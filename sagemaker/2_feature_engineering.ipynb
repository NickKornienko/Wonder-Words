{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a600ccf-4dc0-48d1-b5a7-4025bb6e4c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id, current_timestamp\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04f7b11-a400-4500-b473-72b7093c0f09",
   "metadata": {},
   "source": [
    "# ETL of Large Source Dataset\n",
    "Before uploading the dataset into a database and dataloading pipeline, the data has to be converted into I.I.E (independent and identifiable data) form by including a unique I.D and timestamp for each row. The data will also be split into its' train and validation subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee3aec78-65c0-43d4-ab99-031f28bac78c",
   "metadata": {
    "kernelspec": {
     "display_name": "Python 3 (ipykernel)",
     "language": "python",
     "name": "python3"
    },
    "language_info": {
     "codemirror_mode": {
      "name": "ipython",
      "version": 3
     },
     "file_extension": ".py",
     "mimetype": "text/x-python",
     "name": "python",
     "nbconvert_exporter": "python",
     "pygments_lexer": "ipython3",
     "version": "3.11.11"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for EMR Serverless application state to become STARTED\n",
      "Waiting for EMR Serverless application state to become STARTED\n",
      "Initiating EMR Serverless connection..\n",
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>00fq933kluiv1p0a</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://j-00fq933kluiv1p0a.dashboard.emr-serverless.us-east-1.amazonaws.com/?authToken=eDAxTWBPJDqowRpjbFwrfAAITh5S5nJNjVPFnGEQIHWrES85cCosrXBX0u2P0-CBmhUrsDx1uUqCu7POpneoM6skVX5KkHDKCzS36caGN4c_bts0LtwXH4KLWMcgVmzrGy3CPY1yfMnPvjbObGwAmredPoq-GUY22ekxD6FYpR39gEe-1b-MsvwbaCThlZlFEFPO_oootLMWOpcFQ9DEVsu-Q0WbKg4vI2yHOukqCYO6KfLavJBZWVvP85eXWm6rQIvo_e22evxd-GytNEcL3zmC97gQoLblhj1pcYLKu_PQccGGfWlFSAEeqsg7Ztck72KHXLoqyKD0cxaGP38BpsMaElniWH68d9mXd70jnpDQkKGqjKqHBMmRicpgP9dj8MSegMsyOqgE_7GZxS5dKI8zSfEe2UnS2TLcRiekuZuzPGwhQ-qMIsJ3AMbUc-ymbErNUtcEWGs97MV7v24RkNPvJNz1AklFW2IHWKuEh4c2FPGuHPGSQvqg29tjFSxsiEEmiRV6pLsL_OVsRFyhnBpqw8OdqsMPl4nPFU9d-QS_xEU8JW9rq0wx7vomcEfsJsaFGJdcDqnOd05XZzTnWnKjkh4JKL4E7qOm1gvdxXu7sOu-dYL0GAqlFFotzNH-oavqwazMGYayt3eYu_hHf1LpTfW27z_SWt1C3DIFxo36qGWJCByKlmlHYIh-vlPknD-VXgtng5B1AE7of5EXFWqjAeDbBOHwBQlc_DfJRKe0TJbaTT8kUiPFG6x7FkzbHFf2UXGMUZpCaIothZhfHfuDz8lcGEe-jUYDGgJ97YEpXAKhkgK3Nza-6xm-NQpXryOmIWiHa8KXa3HTdgmn0SI2yxsaa5WYL_HgBZb63DqFOp3t90izgLFz1RKvqoXxPtX3-_6d-40eAGYUvkd_8Ysqu90y8x4C1mt546cQNM_G3w.eyJraWQiOiJBUUlCQUhnTVJ1MlZyR2FFajR3bTc2dEZqMThGMUsyVFdZbkk4WXpxS0lRamg5YTZHZ0hLMCs0VEZmQkFFT0c4SGZWRHFWNERBQUFBb2pDQm53WUpLb1pJaHZjTkFRY0dvSUdSTUlHT0FnRUFNSUdJQmdrcWhraUc5dzBCQndFd0hnWUpZSVpJQVdVREJBRXVNQkVFREVJWGo5TExJTGdCN29pOXJRSUJFSUJibi92c2dJL2VaQitsaHZZQUlFOUJydllKNVhhazlPRjlab1JVYm9NVGtNcnRJVm1uYzFBcUZickIrRUI4RHdiRFFXUkZBaDVoYmh3S0w4TU1uVHVrNVB3ek5jMytKQVZtSFpJY3ZjS1g2NHRQWmdXaXJTQVRNSEZhN0E9PSJ9\">Link</a></td><td><a target=\"_blank\" href=\"https://j-00fq933kluiv1p0a.dashboard.emr-serverless.us-east-1.amazonaws.com/logs/SPARK_DRIVER/stderr.gz?authToken=eDAx3TmshLX-HjTgdFkdYnRlyPTlzj3wzIIXD_K0AQId4-rQdE6uIgfsj4vFJRZeeC1mGSyouaehNlfDe3XmfyG1tIe4P_h0Qm3EC6Bccjbh7w__hAU_eq2SSk7h4MpguWx9aTWx4QxfVYE11kSVox_FMYcpH-tSaDItHHHsCFWZCItgnvlwU9o0UrGfRK5YxKKI7hsGVRKJ8W8kJBQhPMk40aEOvMQUsWZQ6IyDL6G_Yp-xrihSAWCIezQfW3DdX0xLdSh1yVgei_hBhst3vsbR1piaokRQqn3Pm456iQLHeeQUvWVfjqLIz-0_08q9dPsLXibfj5ABZEzbv2kxF2C-KFkxDlYUFBDXEh8EOf6RbsvGt1U1ek3B0epO7PQ3Q9d2i42SICdnYxP3MrZ3Ak68Md1nebespQEADe0oL-9_Mw29SaF4u_pizaZ70fy1f2aV1hzJlTJLR5iB1cUo94obPvtxajttUvtpGKhREk3fo-tbIZuqnGjANWx66qUttAc62e9yGpJy1OTJCTpYwsOewCnXTTkX3eSr2PoKQaBuVv_bWz6x5HKTpJOeTwo3DDYxsDmc8Jq1exJ9fHKOB5YfEiCYdQmnPpE8NKobNlF4OwIiV6ZDw5RLK2D9F4irFz0Y9GVBbj5d_R7dLv75NmSG4svbb8kBjl7pQxQd84TyG-UcnID4PSaE-8I_TuwR8-hOftn92-w6Q35Z7PHTtq84FTRGLynacPi1aY7x67zlHh9DvodvV1Vfs7iieJBBUjgaKvDqdUzuha2gTHY6O_QqOXU0BwwgZYyaOe-E4EDoPaHA9Qb7Jor2TW304xl8nEBGyVkofhpiGHDcxmdusYWAs3V12mIcRDd_HkGxbi6b44MtUL_-SYlEpWzjTSzMrXZ4eN2vD2Z2TJlXmamKX2Ns088GlpuiQvEIKk1phsTFVA.eyJraWQiOiJBUUlCQUhnTVJ1MlZyR2FFajR3bTc2dEZqMThGMUsyVFdZbkk4WXpxS0lRamg5YTZHZ0hLMCs0VEZmQkFFT0c4SGZWRHFWNERBQUFBb2pDQm53WUpLb1pJaHZjTkFRY0dvSUdSTUlHT0FnRUFNSUdJQmdrcWhraUc5dzBCQndFd0hnWUpZSVpJQVdVREJBRXVNQkVFREVJWGo5TExJTGdCN29pOXJRSUJFSUJibi92c2dJL2VaQitsaHZZQUlFOUJydllKNVhhazlPRjlab1JVYm9NVGtNcnRJVm1uYzFBcUZickIrRUI4RHdiRFFXUkZBaDVoYmh3S0w4TU1uVHVrNVB3ek5jMytKQVZtSFpJY3ZjS1g2NHRQWmdXaXJTQVRNSEZhN0E9PSJ9\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%load_ext sagemaker_studio_analytics_extension.magics\n",
    "%sm_analytics emr-serverless connect --application-id 00fq6j1a0fiulq09 --language python --emr-execution-role-arn arn:aws:iam::597161074694:role/service-role/AmazonEMR-ServiceRole-20250211T131858"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a17e73c-7391-4cd9-8653-cd28e43ca836",
   "metadata": {},
   "source": [
    "## Connecting to PySpark\n",
    "Since the data is large (~2 million rows), processing the data in a distributed cluster will speed up processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b86245a5-15df-4d96-ac26-f29deed5e04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/15 00:33:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .config(\"spark.driver.memory\", \"50g\") \\\n",
    "    .appName('spark') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6043e0ef-1539-4bf2-8f25-087e3ed78692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since skeskinen/TinyStories-GPT4 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/sagemaker-user/.cache/huggingface/datasets/skeskinen___tiny_stories-gpt4/default/0.0.0/3c6dc87e4c3c6079b89e9271c5f91d0f226c103c (last modified on Sat Feb 15 00:19:37 2025).\n"
     ]
    }
   ],
   "source": [
    "tinystories = \"skeskinen/TinyStories-GPT4\"\n",
    "train_data = load_dataset(tinystories, revision = 'refs/convert/parquet',split=\"train[:2196080]\")\n",
    "val_data = load_dataset(tinystories, revision = 'refs/convert/parquet', split=\"train[2196080:]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32587d3f-f9a3-4ab1-92cb-5612e7ba9ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['story', 'summary', 'source', 'prompt', 'words', 'features'],\n",
       "    num_rows: 2196080\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2debb412-cb55-4569-9d45-d7aaf6841404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['story', 'summary', 'source', 'prompt', 'words', 'features'],\n",
       "    num_rows: 549020\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2718326e-59a8-4829-a7bd-028e8bdd6ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = spark.createDataFrame(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9f28f9c-bb39-4edb-b7b3-b37b34d9d304",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = spark.createDataFrame(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf28967f-5c81-4d9e-b565-2a92c056e084",
   "metadata": {},
   "source": [
    "## Unique ID and Timestamp\n",
    "Adding these columns will make the records in the dataset independently identifiable for use with a database in AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b096a0e5-382a-4348-9eca-b84fa6c89c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns with PySpark UDFs\n",
    "train_data = train_data.withColumn(\"unique_id\", monotonically_increasing_id()) \n",
    "train_data = train_data.withColumn(\"timestamp\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d68fa4f-0a74-46b9-9fc9-22d148bf8927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns with PySpark UDFs\n",
    "val_data = val_data.withColumn(\"unique_id\", monotonically_increasing_id()) \n",
    "val_data = val_data.withColumn(\"timestamp\", current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7bc3bd-464b-4176-94dc-535f8b1284bd",
   "metadata": {},
   "source": [
    "## Features and Words\n",
    "These columns were originally formatted in array data structures, and need to be converted to string format to allow for use with the training pipeline for language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df455313-023f-44da-8608-bb870d9cc894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_to_string(arr):\n",
    "    kind = 'narrative features: '\n",
    "    return kind + \", \".join(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83520bb0-7a4e-4d04-8c2e-92afafbb0219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "592bede7-0c9d-4527-8b2f-2b0662c1a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_string = udf(features_to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4711979c-4612-42d1-85c8-0f8c904ed148",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.withColumn('string_features', features_to_string(col('features')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9713a6f-43bd-4872-a75a-54aff121c66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = val_data.withColumn('string_features', features_to_string(col('features')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49c09a52-d7de-4fa0-a513-cf92062872e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_string(arr):\n",
    "    kind = 'vocabulary features: '\n",
    "    return kind + \", \".join(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1c85e8d-e95c-4da7-bdab-9cbce9669ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_string = udf(words_to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b30cc16-161d-4381-b1a2-f1f9cdd5b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.withColumn('string_words', words_to_string(col('words')))\n",
    "val_data = val_data.withColumn('string_words', words_to_string(col('words')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b0c06f91-d3b4-4779-b30d-74a864df3f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop('words').drop('features')\n",
    "val_data = val_data.drop('words').drop('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a5d2122e-bac8-4ef7-b329-ec80d2b81379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- prompt: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- story: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unique_id: long (nullable = false)\n",
      " |-- timestamp: timestamp (nullable = false)\n",
      " |-- string_features: string (nullable = true)\n",
      " |-- string_words: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f453d6a2-6274-4e5f-bc70-b1a299679e93",
   "metadata": {},
   "source": [
    "## Uploading to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3f6a9514-25f9-4ff3-92e0-41018ef70d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/15 01:01:09 WARN TaskSetManager: Stage 3 contains a task of very large size (359110 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/02/15 01:01:22 WARN TaskSetManager: Stage 6 contains a task of very large size (359110 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/02/15 01:01:29 WARN PythonRunner: Detected deadlock while completing task 2.0 in stage 6 (TID 14): Attempting to kill Python Worker\n",
      "25/02/15 01:01:29 WARN PythonRunner: Detected deadlock while completing task 1.0 in stage 6 (TID 13): Attempting to kill Python Worker\n",
      "25/02/15 01:01:29 WARN PythonRunner: Detected deadlock while completing task 7.0 in stage 6 (TID 19): Attempting to kill Python Worker\n",
      "25/02/15 01:01:29 WARN PythonRunner: Detected deadlock while completing task 3.0 in stage 6 (TID 15): Attempting to kill Python Worker\n",
      "25/02/15 01:01:29 WARN PythonRunner: Detected deadlock while completing task 5.0 in stage 6 (TID 17): Attempting to kill Python Worker\n",
      "25/02/15 01:01:30 WARN PythonRunner: Detected deadlock while completing task 6.0 in stage 6 (TID 18): Attempting to kill Python Worker\n",
      "25/02/15 01:01:30 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 6 (TID 12): Attempting to kill Python Worker\n",
      "25/02/15 01:01:30 WARN PythonRunner: Detected deadlock while completing task 4.0 in stage 6 (TID 16): Attempting to kill Python Worker\n",
      "25/02/15 01:01:31 WARN TaskSetManager: Stage 12 contains a task of very large size (359110 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/02/15 01:01:53 WARN TaskSetManager: Stage 19 contains a task of very large size (89818 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/02/15 01:01:56 WARN TaskSetManager: Stage 22 contains a task of very large size (89818 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/02/15 01:02:00 WARN PythonRunner: Detected deadlock while completing task 1.0 in stage 22 (TID 53): Attempting to kill Python Worker\n",
      "25/02/15 01:02:00 WARN PythonRunner: Detected deadlock while completing task 3.0 in stage 22 (TID 55): Attempting to kill Python Worker\n",
      "25/02/15 01:02:00 WARN PythonRunner: Detected deadlock while completing task 6.0 in stage 22 (TID 58): Attempting to kill Python Worker\n",
      "25/02/15 01:02:00 WARN PythonRunner: Detected deadlock while completing task 7.0 in stage 22 (TID 59): Attempting to kill Python Worker\n",
      "25/02/15 01:02:00 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 22 (TID 52): Attempting to kill Python Worker\n",
      "25/02/15 01:02:00 WARN PythonRunner: Detected deadlock while completing task 5.0 in stage 22 (TID 57): Attempting to kill Python Worker\n",
      "25/02/15 01:02:00 WARN PythonRunner: Detected deadlock while completing task 4.0 in stage 22 (TID 56): Attempting to kill Python Worker\n",
      "25/02/15 01:02:01 WARN PythonRunner: Detected deadlock while completing task 2.0 in stage 22 (TID 54): Attempting to kill Python Worker\n",
      "25/02/15 01:02:01 WARN TaskSetManager: Stage 28 contains a task of very large size (89818 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#converting to huggingface dataset objects\n",
    "train_data = Dataset.from_spark(train_data)\n",
    "val_data = Dataset.from_spark(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "65a14b74-012a-44e6-a810-1ba604f72f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_splits = {'train': train_data, 'validation': val_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ce311fb3-4857-44e4-aee8-aa9532a2731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = 'Alexis-Az/TinyStories'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "af34bf8b-19b4-48f7-85cd-c9d2b1cef4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = DatasetDict(df_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8d0c8d7c-968c-4e14-baca-f787060b4df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbe26209e9f4954871a9ce36f290aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc0874f148d41f29c68df41292ef603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/314 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187f4d908ccb4f5b9dbeb4a97114fcfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/314 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8751b57a06f04e979eed4d678373acac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/314 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d10daa2a7114be6992098109c342e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/314 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9cfb68451de4cbdab4454fe5edf45f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/314 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a79450fe2548a2a57d2281f49cd70c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/314 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b70d83ac644f5cb7353baa61b9f590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/314 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75853e6465144281827e267c83b1b20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f16f2979be491b967a78997d974ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/275 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "207294cdc7cb473f885590131ab5f66d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/275 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fdeab69ebb44b7e8dfba6602ca3bb95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Alexis-Az/TinyStories/commit/2b8499b9f9d157b30ed23ed91ce9b1360d38a93a', commit_message='Upload dataset', commit_description='', oid='2b8499b9f9d157b30ed23ed91ce9b1360d38a93a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/Alexis-Az/TinyStories', endpoint='https://huggingface.co', repo_type='dataset', repo_id='Alexis-Az/TinyStories'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.push_to_hub(repo_id=repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3741d1-7a32-4c73-b515-68503302b8e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
