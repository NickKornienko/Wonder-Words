{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c24eaf8-2f4e-4bec-89aa-ae6c02ac2430",
   "metadata": {},
   "source": [
    "# Case Study of the'TinyStories' Dataset\n",
    "The 'TinyStories' dataset is a large dataset that was generated using AI story request prompts and responses. It helps facilitate quick fine-tuning of models that were trained with real-world data - such as LLaMA. As this is a large dataset, and this data was generated with the help of AI, the data may have unintentionally been skewed or biased during generation. It is essential to examine the contents of this dataset and whether it is suitable to our applications content-moderation objectives with regard to literacy, mental-health, and creativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "062c2ff4-3e24-475b-95aa-3fbfbb3a284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "176d203f-fb16-4fd2-b16d-f8dad4516505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, countDistinct\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType\n",
    "from pyspark.accumulators import AccumulatorParam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c06983af-d363-42cb-99a0-15879a9e89f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a127284-6e5d-4520-b1fe-17fa615bc395",
   "metadata": {},
   "source": [
    "## Setting up the Distributed Processing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bad87510-bb69-4cad-a5e7-e079a9e1fcd6",
   "metadata": {
    "kernelspec": {
     "display_name": "Python 3 (ipykernel)",
     "language": "python",
     "name": "python3"
    },
    "language_info": {
     "codemirror_mode": {
      "name": "ipython",
      "version": 3
     },
     "file_extension": ".py",
     "mimetype": "text/x-python",
     "name": "python",
     "nbconvert_exporter": "python",
     "pygments_lexer": "ipython3",
     "version": "3.11.11"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for EMR Serverless application state to become STARTED\n",
      "Waiting for EMR Serverless application state to become STARTED\n",
      "Initiating EMR Serverless connection..\n",
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>00frdaescp02ap0a</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://j-00frdaescp02ap0a.dashboard.emr-serverless.us-east-1.amazonaws.com/?authToken=eDAxJ6TeGuSwBDkr_0GkDaFnnVwcARiQkB5qMTl1mTHm-zuuyhniFW7Yqusm8pP5Qc1Y_M1caMnn8DO-N27kV1pKzPTuEKkolnMogxChjHnh-gMHZ10C2sdWeDImfMLIYfrP1oVANgOasFT1G21nkI8i2DVjTkjvH_CIhp3XdOlY8IM8zrdBNA18hlauFVBnxBzurC-BbQIj4F1_mA6MPnYtIehXoVGG-sQDMwzWMa0h39htdWXO0CEYalKp4OETcoSy76Y9IAK0vCwm70tpI4CsFTP4mcTp6Y9zVv2oI5QOJ-1lPn3n9fOoejdsgm9Gv91Q9r9mUclXQ2YVIVrsbIXQqez7ZsWuDWV4aekHuIfT-nGhXlRpY0qcjv0omsjPJ6zRcN2-gul6jD1zhR79JnR_mKAoOnIZlSW0l9L2VW-usDYZqzVzXkgu_B2KUMEgORM2tS7_CSsAlfEOWO72OpWkiE4yjgp41AgS-C9aoTGzD7dpIfgX6WYDXuCdBhay_uagBkbIck6Yuztn3Iri6q0nlr8M95HWcOWFl5jgBuMGwXODUTxDonc39jUebeHj-wrGF4CRPdeVaI--_swUBAXqK7lJZdtu5nL3vst7yzyGnGbhthpRhOtO3dEnit5PEp7d1797C4KcEENelyWzNxAv0c7cJgGe1fmExSfNZP36EiMJ9_bszaT0QLf5j-wWGQ3ofmYew0K9xpsvZeikXbmHy074cNMr1UkMXzC7IeTY0zICBv-N02cX9IbfLWQxbiR5g_E-v4X0SB-jpMLGA7s74VovQKGiSwkZr9uLVqwjBguk-SW38zTeswUfgA5JHGeWBdRLpCGa8JRL7CNQWOO9G2rc2V7s9CboKYfzrfuMCnL0WdBKHr0lnTETbbBEFWG4nvpcJ6ZUHb5eHIpW4UnVoyGem4MOjl__MNgpdCtfzw.eyJraWQiOiJBUUlCQUhnTVJ1MlZyR2FFajR3bTc2dEZqMThGMUsyVFdZbkk4WXpxS0lRamg5YTZHZ0hvbXY5b3hWWTNNMFc0dXo1TzZSK1hBQUFBb2pDQm53WUpLb1pJaHZjTkFRY0dvSUdSTUlHT0FnRUFNSUdJQmdrcWhraUc5dzBCQndFd0hnWUpZSVpJQVdVREJBRXVNQkVFRE5ZVk5EY3V3Z1B4K3dsK0pnSUJFSUJiQ0VyOXBLTDNiY0JoWjF1dURqTWxEMVBJcS9ZUXNoKy9Kd0VGY2FPS3JMbHZDZlNtNmFMOWc2QldqdndXdmdCNGx0QXRZTTFaaVRsZzhOTnRUY3FlTkM0Ymx6eVZWZHQvTzZVRTBCNlphZUxjSlBDNFVOc1NJQXU1K3c9PSJ9\">Link</a></td><td><a target=\"_blank\" href=\"https://j-00frdaescp02ap0a.dashboard.emr-serverless.us-east-1.amazonaws.com/logs/SPARK_DRIVER/stderr.gz?authToken=eDAxjQQj92TxEpzRjAj6fDnmo4fFlPgWi-nxU7lAumxarhv1crKG1KYzR4t6iw0cNRHVtvDYWItefzZ855EVbgruBKjFrmKDMK9k2f6v4oPmcurULdoCyVGGeSZ3yt_Uy7cQsW3Nx9HisViJRbKWsnY9kw4ND9ObPCUKUVP1PNSWkgNCus-rAff9ZqDdn-oIW0H1id8BnF3pE3_vrSohUhMcmYnSPgv8iRRiGM4P7aKble1B1PCxjDdXhfEJqxpPTx_xBpq-ctMtRarqAxWbKd1AVwKVcmMgR7OwGLp29hKajGMcm8ZFxbdNO28_fRO0_iPSK9lR83PGtkcUjCjIN6bz2SkOrCVSIG3b2G2nfesopSv9d99saT7rHjzMc-59QtFc2UOBTcaeJXyAGei4rZlZme1TdNjMnFLxkRzZiWNL8NiVV6JDeCDVbyn_k2yEXoPstixeOmv1YgKAjGArHpKUvbSSHkfFwP0FwHZ4sIjdAANwpnKQiX91o3LHKrVMLFu9r8c7XI7zX6nQX8vHT6fW7OdWqrsEZXNdJ5VFjReYweRoTZ_C5Tl4X2pKd9nNpnGGKrdCKli2xHTodrnlXLMXvqIwu9-dBhvczWcUZABu5qtJi3eZoUxSdUNclZ9yTfwgfNYZPIKtNDPDOx1Sz0fJ4BTShLF_d--TYMeaFl6x_VybsIteFkp60emxbqQ3jnCvPCycof407Eyrik2V1mJqfP6fPJNwTCEjvA2WOo8YkfccAwG2Fn4I_chd8KEuZGJjcTRVMZATrs9bPo96z8KJQaDczZXVvtD7lzTcfnzMoPGtR-OHKdkLCPHFY49iQqZuHcR75aCxdL7QhMq8BaHkdLtVDAi6kwPxnJlITup80MObm4HBQxcqVTyVhke1YY28ReSx8dt3Vwxrcx_Zhypxu7juJiSJmVLlijPPxm8n2w.eyJraWQiOiJBUUlCQUhnTVJ1MlZyR2FFajR3bTc2dEZqMThGMUsyVFdZbkk4WXpxS0lRamg5YTZHZ0VFS1ZZdEtDMGtTSkNic1l1bkVmSGxBQUFBb2pDQm53WUpLb1pJaHZjTkFRY0dvSUdSTUlHT0FnRUFNSUdJQmdrcWhraUc5dzBCQndFd0hnWUpZSVpJQVdVREJBRXVNQkVFREFoeEIyZW03Y2dnR2FFdWlBSUJFSUJiQmJLNjNJbVVpNFdoT2dXQ3hGQVJxOHRKWG84QnpISlIvVy8wSTM2V3B5bUFBenpNVmtKNU9uYW56RHkreEI4SlgyVDNCcGsyTUpWVVN0UXJsRTdhOXNWbEJsK0w5dnNwR0o5OEhDUU4zcm1rQXM4U2FTTWV0eGRyZWc9PSJ9\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7403da6d52149238137ca41952f2815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%load_ext sagemaker_studio_analytics_extension.magics\n",
    "%sm_analytics emr-serverless connect --application-id 00fra2001bfrlm09 --language python --emr-execution-role-arn arn:aws:iam::597161074694:role/service-role/AmazonEMR-ServiceRole-20250211T131858"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4d6dda3-922e-4a43-bd46-44fc1f95060a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/01 01:10:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# connecting to the spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .config(\"spark.driver.memory\", \"64g\") \\\n",
    "    .appName('spark') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a931d-75ee-47a5-b493-b699924dde3b",
   "metadata": {},
   "source": [
    "# Statistical Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76440b2b-338d-4c36-970a-a24e86d3ca5b",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7035db01-cc5e-4af1-975b-81b60cf42655",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"skeskinen/TinyStories-GPT4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11f199f8-6faf-4c7c-9312-2d50435d3f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['story', 'summary', 'source', 'prompt', 'words', 'features'],\n",
       "        num_rows: 2745100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9120ad9e-80ea-449b-9132-8cfc2ff045a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spark.createDataFrame(data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8527681c-8922-407b-a209-5e471cf09616",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 8  # or another appropriate number\n",
    "train = spark.createDataFrame(data['train']).repartition(num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0425f99-27c4-417b-bd3f-3a524f3f337d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/01 01:48:01 WARN TaskSetManager: Stage 1 contains a task of very large size (223671 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 1:>                                                        (0 + 16) / 16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=====================================================>   (15 + 1) / 16]"
     ]
    }
   ],
   "source": [
    "# Verify the number of partitions\n",
    "print(f\"Number of partitions: {train.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df75014-1d5f-4484-8ecc-7dbd649859c6",
   "metadata": {},
   "source": [
    "## Mental Health\n",
    "As for mental-health, goals may be defined in objective terms by inferring the emotions, subjects, topics, genres, or any commonly used keywords related to mental-health within the data. For this purpose, we can use the list of words and narrative features provided within the prompts provided to the AI in the training data for each row in the dataset. By counting the occurences of the 'words' and 'features' columns used within the prompt, we can make some insights of the mental-health objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d61094c5-1189-48aa-bc05-34a79f867ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordCountAccumulator(AccumulatorParam):\n",
    "    def zero(self, value):\n",
    "        return {}\n",
    "\n",
    "    def addInPlace(self, v1, v2):\n",
    "        # v1 is the accumulator's current state\n",
    "        # v2 is the new dictionary being added from a partition\n",
    "        for word, count in v2.items():\n",
    "            if word is not None and word.strip():\n",
    "                v1[word] = v1.get(word, 0) + count\n",
    "        return v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8dedae9d-3f0d-4cad-bd6c-14015dcbe7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_count_unique(strings):\n",
    "    if strings is None:\n",
    "        return None\n",
    "    normalized_strings = [s.strip().lower() for s in strings if s is not None and s.strip()]\n",
    "    return normalized_strings\n",
    "# the sparkerized function to normalize the words/features for comparison\n",
    "normalize_count_udf = udf(normalize_and_count_unique, ArrayType(StringType())) # Assuming string type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "38106c55-c149-4c3b-8a3d-0ba335295174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the accumulator across partitions of the dataset based on col type\n",
    "def process_features_partition(partition_iterator, col_type):\n",
    "    # Initialize local counter for this partition\n",
    "    partition_features = {}\n",
    "    \n",
    "    # Process each row in the partition\n",
    "    for row in partition_iterator:\n",
    "        features = row[\"normalized_\" + col_type]\n",
    "        if features is not None:\n",
    "            # Handle both string and list cases\n",
    "            words = features.split() if isinstance(features, str) else features\n",
    "            \n",
    "            # Count words in this row\n",
    "            for word in words:\n",
    "                if word and word.strip():\n",
    "                    word = word.strip()\n",
    "                    partition_features[word] = partition_features.get(word, 0) + 1\n",
    "    \n",
    "    # Add the partition counts to the accumulator\n",
    "    if partition_features and col_type == 'features':\n",
    "        features_accumulator.add(partition_features)\n",
    "    elif partition_features and col_type == 'words':\n",
    "        words_accumulator.add(partition_features)\n",
    "    \n",
    "    # Return the iterator for the partition\n",
    "    return iter([1])  # Return dummy value to force evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a009cc-b815-46a6-8035-8f19df9a09a6",
   "metadata": {},
   "source": [
    "### Narrative Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3fafc262-54fc-4e58-85b8-e61ed5952ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_accumulator = spark.sparkContext.accumulator(\n",
    "    {}, WordCountAccumulator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc876450-a892-490c-b7b2-837d480c0056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure processed as lists of words\n",
    "train = train.withColumn(\"normalized_features\", normalize_count_udf(col(\"features\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d70698a4-77e7-4fe7-9c9e-045e016ce9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Ensure proper partitioning\n",
    "num_partitions = 200  # Adjust based on your cluster size\n",
    "train = train.repartition(num_partitions)\n",
    "\n",
    "# Force evaluation and verify processing\n",
    "col_type = \"features\"  # or whatever column name you want to process\n",
    "total_partitions = train.rdd.mapPartitions(\n",
    "    lambda partition: process_features_partition(partition, col_type)\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4cf566c7-7e6d-412c-8c13-174ac803d3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dialogue': 1470404,\n",
       " 'foreshadowing': 250789,\n",
       " 'twist': 539383,\n",
       " 'moralvalue': 274152,\n",
       " 'conflict': 250696,\n",
       " 'badending': 250300}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the final word counts\n",
    "features_accumulator.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fbf14c1b-3bdf-4412-b33b-3d2124ba357f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dialogue         1470404\n",
       "foreshadowing     250789\n",
       "twist             539383\n",
       "moralvalue        274152\n",
       "conflict          250696\n",
       "badending         250300\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_occurences = pd.Series(features_accumulator.value)\n",
    "feature_occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "765b95dc-864c-46b1-b748-c26d23dcdafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6.000000e+00\n",
       "mean     5.059540e+05\n",
       "std      4.859297e+05\n",
       "min      2.503000e+05\n",
       "25%      2.507192e+05\n",
       "50%      2.624705e+05\n",
       "75%      4.730752e+05\n",
       "max      1.470404e+06\n",
       "dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_occurences.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308dc61e-afde-4215-83ed-f19f66d71218",
   "metadata": {},
   "source": [
    "### Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bf71fa2e-0088-42cc-9f25-76a349eb8c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_accumulator = spark.sparkContext.accumulator(\n",
    "    {}, WordCountAccumulator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d6d20150-8f57-48f4-9344-0c321ab5de27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.withColumn(\"normalized_words\", normalize_count_udf(col(\"words\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b578fb80-c7f7-4fba-8eae-9b928be416e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Force evaluation and verify processing\n",
    "col_type = \"words\"  # or whatever column name you want to process\n",
    "total_partitions = train.rdd.mapPartitions(\n",
    "    lambda partition: process_features_partition(partition, col_type)\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d2737b45-5d55-43eb-891a-ea43257b740b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reply      7065\n",
       "engine     2509\n",
       "bald      11245\n",
       "feed       7070\n",
       "lawyer     2529\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_occurrences = pd.Series(words_accumulator.value)\n",
    "word_occurrences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "47d948c7-4e49-456b-8f4a-2c1ecdacb2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     1603.000000\n",
       "mean      5137.429819\n",
       "std       3471.319457\n",
       "min       2418.000000\n",
       "25%       2563.000000\n",
       "50%       2623.000000\n",
       "75%       7031.000000\n",
       "max      20563.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_occurrences.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "83ade00e-9050-44c5-a976-0ccc465cdc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd998ba-ff46-4b64-8e2b-ee6204b0f96c",
   "metadata": {},
   "source": [
    "## Creativity\n",
    "Objectively speaking, creativity is a hard goal to define as it can be objectively defined in a multitude of way, as are the aforementioned topics of literacy and mental-health. However, many people might agree that creativity is somehow unique. Therefore, it may be possible to define the goal of creativity by understanding the level of variance in the models responses to similar prompts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefb81cf-68dc-4308-bb0b-0a0458b37be1",
   "metadata": {},
   "source": [
    "# Statistical Analysis\n",
    "In this section the statistical metrics calculated from the training data is analyzed and visualized for making insights with regards to the stated objectives. Furthermore, given the complexity in understanding the level of literacy from the data - as no 'literacy level' column is within the data - a language model is used to classify the responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222ecc21-098d-496b-80bb-eca1404a95c0",
   "metadata": {},
   "source": [
    "## Literacy\n",
    "WonderWords' literacy goals may be defined in objective terms by classifying whether the responses are at a lower or a higher reading level. As our application is targeting a youth demographic, utilizing the categorization system used in most libraries and school systems will help illustrate whether the training data is biased towards a specific set of reading levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d6c85e-b99d-4ff3-9be2-c9bc02a020dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6807b95c-f612-4cd4-8d4a-efc8b2f12962",
   "metadata": {},
   "source": [
    "## Mental Health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8cad3c-88a6-4591-9600-a2bbc899f490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d33ce727-9f78-4d50-81f5-f7825acbcdc9",
   "metadata": {},
   "source": [
    "## Creativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd172f7b-6208-4017-adad-c53ceb4909fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
