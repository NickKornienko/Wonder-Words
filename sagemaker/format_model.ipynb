{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e81b7fea-9263-4320-b855-33a45ed11058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 18:38:18.953625: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaab31d5-4c6f-4b1b-9c93-8da22b4daf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 23:43:17.462922: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO:matplotlib.font_manager:generated new fontManager\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.serve.builder.model_builder import ModelBuilder\n",
    "from sagemaker.serve.builder.schema_builder import SchemaBuilder\n",
    "from safetensors.torch import load_file\n",
    "import torch\n",
    "from sagemaker.serve.mode.function_pointers import Mode\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "from sagemaker.serve import InferenceSpec\n",
    "import boto3\n",
    "from sagemaker.serve import CustomPayloadTranslator\n",
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "from sagemaker.serve.spec.inference_spec import InferenceSpec\n",
    "from transformers import pipeline\n",
    "from sagemaker.serve import ModelServer\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "import sagemaker\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "sagemaker_session = Session()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# get execution role\n",
    "# please use execution role if you are using notebook instance or update the role arn if you are using a different role\n",
    "execution_role = get_execution_role() if get_execution_role() is not None else \"your-role-arn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acafddc7-e998-4229-869f-95b9fc919d7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munsloth\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/unsloth/__init__.py:91\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# First check if CUDA is available ie a NVIDIA GPU is seen\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Fix Xformers performance issues since 0.0.25\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimportlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!"
     ]
    }
   ],
   "source": [
    "import unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d0764d-9824-4b80-a806-ece2c3d5c5ec",
   "metadata": {},
   "source": [
    "## Loading the Finetuned Model\n",
    "SAVED_MODEL is the lora huggingface repo of the model that has been fine-tuned. INFER_MODEL is the huggingface repo of the model that has the lora weights merged with the base weights for inferring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ccd85c0-1094-4fbe-b66e-dc94b47356dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_MODEL = \"Alexis-Az/Story-Generation-LlaMA-3.1-8B-10k\"\n",
    "INFER_MODEL= \"Alexis-Az/Story-Generation-Model\"\n",
    "max_seq_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edc4f67f-d59e-4802-8f21-5ed6330484d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unsloth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m adapter_model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43munsloth\u001b[49m\u001b[38;5;241m.\u001b[39mFastLanguageModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(SAVED_MODEL, load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unsloth' is not defined"
     ]
    }
   ],
   "source": [
    "adapter_model, tokenizer = unsloth.FastLanguageModel.from_pretrained(SAVED_MODEL, load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dc74521-7837-466e-9dfa-b59f315a9d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 15.61 out of 30.89 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 13/32 [00:01<00:01, 14.00it/s]\n",
      "We will save to Disk and not RAM now.\n",
      "100%|██████████| 32/32 [00:14<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "adapter_model.save_pretrained_merged(\"Story-Generation-LlaMA-3.1-8B-10k\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5bedc2-d549-430c-808d-19caa40d811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edae5336-6052-4fbf-a959-433cf54c0a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Finetuning Story Generation Model.ipynb'   hf_auth.ipynb\n",
      " README.md\t\t\t\t    inference_container\n",
      " Story-Generation-LlaMA-3.1-8B-10k\t    load_data.ipynb\n",
      " feature_engineering.ipynb\t\t    unsloth_compiled_cache\n",
      " format_pth_model.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93e13d2-d4e2-4ebe-8f8f-2bbc5491600f",
   "metadata": {},
   "source": [
    "## Saving the Merged Model \n",
    "The model will be pushed to the INFER_MODEL huggingface repo, and will also be saved as a model .pth file for use in the container for inferrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6831cbea-cdb5-4533-98e4-f19190c54d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db7a776a30b149278b3ee853972b08e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pth_model = AutoModelForCausalLM.from_pretrained(\"./Story-Generation-LlaMA-3.1-8B-10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6297c232-6409-48f9-af07-52c67e783b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./Story-Generation-LlaMA-3.1-8B-10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c3be920-007f-45c3-a4cc-ca752325edd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dcc1c70246e4973b222ccc8e96d7d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Alexis-Az/Story-Generation-Model/commit/785a0842eca3e73c6ff7594dc7d1574f78353ce3', commit_message='Upload LlamaForCausalLM', commit_description='', oid='785a0842eca3e73c6ff7594dc7d1574f78353ce3', pr_url='https://huggingface.co/Alexis-Az/Story-Generation-Model/discussions/3', repo_url=RepoUrl('https://huggingface.co/Alexis-Az/Story-Generation-Model', endpoint='https://huggingface.co', repo_type='model', repo_id='Alexis-Az/Story-Generation-Model'), pr_revision='refs/pr/3', pr_num=3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pth_model.push_to_hub(\n",
    "    INFER_MODEL,\n",
    "    tokenizer=tokenizer,\n",
    "    safe_serialization=True,\n",
    "    create_pr=True,\n",
    "    max_shard_size=\"3GB\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f50fabb3-d1d5-4bd3-8d18-240b9563f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "model_dir = \"./serve\"\n",
    "!mkdir -p {model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f5a05be-c7b3-4d20-8345-ae8b7a2f047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pth_model.state_dict(), model_dir + '/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a5b05cd-d5c8-4896-81da-1a42206992f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Finetuning Story Generation Model.ipynb'   inference_container\n",
      " README.md\t\t\t\t    load_data.ipynb\n",
      " deploy_model.ipynb\t\t\t    sample_input.txt\n",
      " feature_engineering.ipynb\t\t    unsloth_compiled_cache\n",
      " format_pth_model.ipynb\t\t\t    working_dir\n",
      " hf_auth.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8e8f3-1f15-4558-82ae-e1cd4022a7da",
   "metadata": {},
   "source": [
    "# Using Sagemaker's 'ModelBuilder' Class to Format the PyTorch Model Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c992ae-e05e-448a-bdff-9a860776f70d",
   "metadata": {},
   "source": [
    "## Inference Object for Handling Input/Output of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df6a19d2-01ef-40e2-ba30-125570b7653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceSpec(InferenceSpec):\n",
    "    def invoke(self, input_object: object, model: object):       \n",
    "        with torch.no_grad():\n",
    "            output = model(input_object)\n",
    "        return output\n",
    "        \n",
    "    def load(self, model_dir: str):\n",
    "        model = AutoModelForCausalLM\n",
    "        model.load_state_dict(torch.load(model_dir+'/model.pth'))\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "inf_spec = InferenceSpec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f534f2d7-fddd-4264-8b7a-5e1e5d9eec2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py310\n"
     ]
    }
   ],
   "source": [
    "instance_type = 'ml.g5.2xlarge'\n",
    "image = image_uris.retrieve(region=region, framework='pytorch', image_scope='inference', version='2.0.0', base_framework_version='pytorch2.0.0', instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ced1a32-2e8d-405b-8d1d-22f4e39f46c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "value: str = \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\"\n",
    "schema = SchemaBuilder(value,\n",
    "            {\"generated_text\": \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\\\nDaniel: Hello, Girafatron!\\\\nGirafatron: Hi, Daniel. I was just thinking about how magnificent giraffes are and how they should be worshiped by all.\\\\nDaniel: You and I think alike, Girafatron. I think all animals should be worshipped! But I guess that could be a bit impractical...\\\\nGirafatron: That\\'s true. But the giraffe is just such an amazing creature and should always be respected!\\\\nDaniel: Yes! And the way you go on about giraffes, I could tell you really love them.\\\\nGirafatron: I\\'m obsessed with them, and I\\'m glad to hear you noticed!\\\\nDaniel: I\\'\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f5a805e-15d3-4831-94be-2da7e54e6396",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./serve\"\n",
    "local_model_dir = str(Path(model_dir).resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1747df8-d6bb-41f2-95c4-b274c2ccdb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_builder = ModelBuilder(\n",
    "    mode=Mode.SAGEMAKER_ENDPOINT,\n",
    "    inference_spec=inf_spec,\n",
    "    model_path=model_dir,\n",
    "    role_arn=execution_role,\n",
    "    image_uri=image,\n",
    "    schema_builder=schema,\n",
    "    env_vars={\"HF_TASK\":\"text-generation\"},\n",
    "    model_server=ModelServer.TORCHSERVE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e19c522-6fd6-48f6-b076-dd1a672bd37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelBuilder(model_path='./working_dir/models/serve', role_arn='arn:aws:iam::597161074694:role/service-role/SageMaker-ExecutionRole-20250210T145384', sagemaker_session=None, name='model-name-0f25e088efd611efa4457e001caa723d', mode=<Mode.SAGEMAKER_ENDPOINT: 3>, shared_libs=[], dependencies={'auto': False}, env_vars={'HF_TASK': 'text-generation'}, log_level=10, content_type=None, accept_type=None, s3_model_data_url=None, instance_type='ml.c5.xlarge', schema_builder=SchemaBuilder(\n",
       "input_serializer=<sagemaker.base_serializers.StringSerializer object at 0x7f8b93abbc90>\n",
       "output_serializer=<sagemaker.serve.builder.schema_builder.JSONSerializerWrapper object at 0x7f8b92a57b50>\n",
       "input_deserializer=<sagemaker.base_deserializers.StringDeserializer object at 0x7f8ba0770950>\n",
       "output_deserializer=<sagemaker.base_deserializers.JSONDeserializer object at 0x7f8b92a57c10>), model=None, inference_spec=<__main__.InferenceSpec object at 0x7f8ff5c30390>, image_uri='763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:2.0.0-gpu-py310', image_config=None, vpc_config=None, model_server=<ModelServer.TORCHSERVE: 1>, model_metadata=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57120ec-1e52-4756-a6e9-b3c8a3ef112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_model = model_builder.build(role_arn=execution_role, sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "266a71e7-31fc-4bd7-9ca7-915c797323bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory = \"./serve\"  # Replace with the actual directory path\n",
    "output_file = \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd6afbc5-579b-4382-92d0-bb36864090d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar --use-compress-program=\"pigz --best --recursive\" -cf 'model.tar.gz' './serve'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8187bc48-c8b7-4c59-b8a9-2bc197e84b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File './model.tar.gz' uploaded to S3 bucket 'unsloth-finetuned' as 'model.tar.gz'\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# 1. Create an S3 client\n",
    "s3 = boto3.client('s3', region_name=region)  # Replace 'YOUR_REGION' with your bucket's region\n",
    "\n",
    "# 2. Define file and bucket details\n",
    "file_path = './model.tar.gz'   # Path to the local file you want to upload\n",
    "bucket_name = 'unsloth-finetuned'      # Name of your S3 bucket\n",
    "object_key = 'model.tar.gz'  # Desired key (filename) for the object in S3\n",
    "\n",
    "try:\n",
    "    # 3. Upload the file\n",
    "    s3.upload_file(file_path, bucket_name, object_key)\n",
    "    print(f\"File '{file_path}' uploaded to S3 bucket '{bucket_name}' as '{object_key}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ddca7-d9fd-4cfa-82a0-edaaf5960941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
