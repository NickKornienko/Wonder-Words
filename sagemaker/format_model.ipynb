{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e81b7fea-9263-4320-b855-33a45ed11058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "#from peft import LoraConfig, get_peft_model\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaab31d5-4c6f-4b1b-9c93-8da22b4daf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-22 23:40:15.648529: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-22 23:40:15.660319: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-22 23:40:15.677640: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-22 23:40:15.683150: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-22 23:40:15.695406: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.serve.builder.model_builder import ModelBuilder\n",
    "from sagemaker.serve.builder.schema_builder import SchemaBuilder\n",
    "from safetensors.torch import load_file\n",
    "import torch\n",
    "from sagemaker.serve.mode.function_pointers import Mode\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "from sagemaker.serve import InferenceSpec\n",
    "import boto3\n",
    "from sagemaker.serve import CustomPayloadTranslator\n",
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "from sagemaker.serve.spec.inference_spec import InferenceSpec\n",
    "from transformers import pipeline\n",
    "from sagemaker.serve import ModelServer\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "import sagemaker\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "sagemaker_session = Session()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# get execution role\n",
    "# please use execution role if you are using notebook instance or update the role arn if you are using a different role\n",
    "execution_role = get_execution_role() if get_execution_role() is not None else \"your-role-arn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1ee75f0-b878-491a-a0fc-429f91be8c29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#!pip install -U accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acafddc7-e998-4229-869f-95b9fc919d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ccd85c0-1094-4fbe-b66e-dc94b47356dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_MODEL = \"Alexis-Az/Story-Generation-LlaMA-3.1-8B-10k\"\n",
    "INFER_MODEL= \"Alexis-Az/Story-Generation-Model\"\n",
    "max_seq_length = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d0764d-9824-4b80-a806-ece2c3d5c5ec",
   "metadata": {},
   "source": [
    "## Loading the Finetuned Model\n",
    "SAVED_MODEL is the lora huggingface repo of the model that has been fine-tuned. INFER_MODEL is the huggingface repo of the model that has the lora weights merged with the base weights for inferring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc4f67f-d59e-4802-8f21-5ed6330484d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapter_model, tokenizer = unsloth.FastLanguageModel.from_pretrained(SAVED_MODEL, load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc74521-7837-466e-9dfa-b59f315a9d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapter_model.save_pretrained_merged(\"Story-Generation-LlaMA-3.1-8B-10k\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5bedc2-d549-430c-808d-19caa40d811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapter_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edae5336-6052-4fbf-a959-433cf54c0a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Finetuning Story Generation Model.ipynb'   inference_container\n",
      " README.md\t\t\t\t    load_data.ipynb\n",
      " feature_engineering.ipynb\t\t    sample_input.txt\n",
      " format_model.ipynb\t\t\t    unsloth_compiled_cache\n",
      " hf_auth.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93e13d2-d4e2-4ebe-8f8f-2bbc5491600f",
   "metadata": {},
   "source": [
    "## Saving the Merged Model \n",
    "The model will be pushed to the INFER_MODEL huggingface repo, and will also be saved as a model .pth file for use in the container for inferrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6831cbea-cdb5-4533-98e4-f19190c54d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_model = AutoModelForCausalLM.from_pretrained(\"./Story-Generation-LlaMA-3.1-8B-10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6297c232-6409-48f9-af07-52c67e783b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./Story-Generation-LlaMA-3.1-8B-10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3be920-007f-45c3-a4cc-ca752325edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_model.push_to_hub(\n",
    "    INFER_MODEL,\n",
    "    tokenizer=tokenizer,\n",
    "    safe_serialization=True,\n",
    "    create_pr=True,\n",
    "    max_shard_size=\"3GB\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051be26b-56b3-40b1-ac3b-8e3a94ece665",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(INFER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50fabb3-d1d5-4bd3-8d18-240b9563f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "model_dir = \"./serve\"\n",
    "!mkdir -p {model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5a05be-c7b3-4d20-8345-ae8b7a2f047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pth_model.state_dict(), model_dir + '/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5b05cd-d5c8-4896-81da-1a42206992f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d686887e-932c-45a7-9f61-f342b6bd64dd",
   "metadata": {},
   "source": [
    "## Scripting the Model as a .pth TorchScript file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22b860c4-91aa-4f37-ba3c-b39f4a10b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pth_model, tokenizer = unsloth.FastLanguageModel.from_pretrained(INFER_MODEL, load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b1f44ff-f698-4ce5-a99b-df39c1e68d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unsloth.FastLanguageModel.for_inference(pth_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "231f8d96-5683-4604-9e67-4bcf7d0b6c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb13de09-2bc4-4378-8144-269b1cb3e723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from unsloth.chat_templates import get_chat_template\n",
    "#tokenizer = get_chat_template(\n",
    "#    tokenizer,\n",
    "#    chat_template = \"llama-3.1\",\n",
    "#    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1768f542-3a19-4c19-9047-9152256342ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_streamer = TextStreamer(tokenizer)\n",
    "#messages = [\n",
    "#    {\"from\": \"human\", \"value\": \"tell me a story about a cute park in california\"},\n",
    "#]\n",
    "#inputs = tokenizer.apply_chat_template(messages, tokenize = True, add_generation_prompt = True, return_tensors = \"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce8942a4-a882-4d2d-83d9-d88babd7bf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_ = pth_model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f56df10a-d576-4a23-81a0-373cd7b73508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pth_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60577a31-c982-43f9-b372-23f215999f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(INFER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86d55bf5-d60c-420a-a546-825bced9176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMMY_TEXT = 'tell me a story about a cute park in california'\n",
    "tok_text = tokenizer.tokenize(DUMMY_TEXT)\n",
    "tokens = tokenizer.encode(DUMMY_TEXT)\n",
    "tok_arr = np.asarray(tokens).reshape(1, -1).astype(np.int32)\n",
    "tok_arr = np.pad(tok_arr, [(0,0), (0, 1024-tok_arr.shape[1])])\n",
    "tok_arr = np.repeat(tok_arr, 2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "183773c1-c80d-49e5-960a-3d5b350aa232",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TritonWrapper(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name: str):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torchscript=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            trust_remote_code=True,\n",
    "            device_map='auto'\n",
    "            )\n",
    "        #self.model = torch.nn.DataParallel(self.model)\n",
    "        #self.model.to('cuda:0')\n",
    "        self.model.output_hidden_states = False\n",
    "                \n",
    "    def forward(self, tokens_tensor):\n",
    "        self.model.eval()\n",
    "        o = self.model(tokens_tensor, output_hidden_states=False)\n",
    "        return o[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a10f08d-5103-4a01-b5b4-80f1bf9674b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8bb906f9b824f09a4499adaa5199615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TritonWrapper(\n",
       "  (model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TritonWrapper(INFER_MODEL)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce9fb539-48a0-4bed-8126-5445daac1627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:729: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traced_model done\n"
     ]
    }
   ],
   "source": [
    "assert model.training == False\n",
    "with torch.no_grad():\n",
    "    tokens_tensor = torch.from_numpy(tok_arr).to('cuda:0')\n",
    "    o = model(tokens_tensor)\n",
    "    traced_model = torch.jit.trace(model, [tokens_tensor])\n",
    "    print(\"traced_model done\")\n",
    "    torch.jit.save(traced_model, f\"{INFER_MODEL.split('/')[-1]}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bba22d17-2c6f-4897-bc43-149928cf5993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./serve/Story-Generation-Model.pt'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = \"./serve\"\n",
    "os.mkdir(model_dir)\n",
    "shutil.move(f\"{INFER_MODEL.split('/')[-1]}.pt\", model_dir + f\"/{INFER_MODEL.split('/')[-1]}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8e8f3-1f15-4558-82ae-e1cd4022a7da",
   "metadata": {},
   "source": [
    "# Using Sagemaker's 'ModelBuilder' Class to Format the PyTorch Model Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd57f3-64d4-409f-875d-0c547e4111c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26c992ae-e05e-448a-bdff-9a860776f70d",
   "metadata": {},
   "source": [
    "## Inference Object for Handling Input/Output of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df6a19d2-01ef-40e2-ba30-125570b7653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceSpec(InferenceSpec):\n",
    "    def invoke(self, input_object: object, model: object):       \n",
    "        with torch.no_grad():\n",
    "            output = model(input_object)\n",
    "        return output\n",
    "        \n",
    "    def load(self, model_dir: str):\n",
    "        model = AutoModelForCausalLM\n",
    "        model.load_state_dict(torch.load(model_dir+'/model.pth'))\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "inf_spec = InferenceSpec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9939bc0e-fef4-4830-b46e-5cd89ac9b8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1.post300'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f534f2d7-fddd-4264-8b7a-5e1e5d9eec2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py311\n"
     ]
    }
   ],
   "source": [
    "instance_type = 'ml.g4dn.12xlarge'\n",
    "image = image_uris.retrieve(region=region, framework='pytorch', image_scope='inference', version='2.3', base_framework_version='pytorch2.3', instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ced1a32-2e8d-405b-8d1d-22f4e39f46c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "value: str = \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\"\n",
    "schema = SchemaBuilder(value,\n",
    "            {\"generated_text\": \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\\\nDaniel: Hello, Girafatron!\\\\nGirafatron: Hi, Daniel. I was just thinking about how magnificent giraffes are and how they should be worshiped by all.\\\\nDaniel: You and I think alike, Girafatron. I think all animals should be worshipped! But I guess that could be a bit impractical...\\\\nGirafatron: That\\'s true. But the giraffe is just such an amazing creature and should always be respected!\\\\nDaniel: Yes! And the way you go on about giraffes, I could tell you really love them.\\\\nGirafatron: I\\'m obsessed with them, and I\\'m glad to hear you noticed!\\\\nDaniel: I\\'\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f5a805e-15d3-4831-94be-2da7e54e6396",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_dir = str(Path(model_dir).resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b562c1de-3cc8-4458-af62-ca5d211deb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "package_dir = 'model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1747df8-d6bb-41f2-95c4-b274c2ccdb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_builder = ModelBuilder(\n",
    "    mode=Mode.SAGEMAKER_ENDPOINT,\n",
    "    inference_spec=inf_spec,\n",
    "    model_path=model_dir,\n",
    "    role_arn=execution_role,\n",
    "    image_uri=image,\n",
    "    schema_builder=schema,\n",
    "    env_vars={\"HF_TASK\":\"text-generation\"},\n",
    "    model_server=ModelServer.TORCHSERVE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e19c522-6fd6-48f6-b076-dd1a672bd37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelBuilder(model_path='./serve', role_arn='arn:aws:iam::597161074694:role/service-role/SageMaker-ExecutionRole-20250210T145384', sagemaker_session=None, name='model-name-5d63d880f17611ef906f2e28d4a0fa8a', mode=<Mode.SAGEMAKER_ENDPOINT: 3>, shared_libs=[], dependencies={'auto': False}, env_vars={'HF_TASK': 'text-generation'}, log_level=10, content_type=None, accept_type=None, s3_model_data_url=None, instance_type='ml.c5.xlarge', schema_builder=SchemaBuilder(\n",
       "input_serializer=<sagemaker.base_serializers.StringSerializer object at 0x7f3b816e4710>\n",
       "output_serializer=<sagemaker.serve.builder.schema_builder.JSONSerializerWrapper object at 0x7f3aa2e40990>\n",
       "input_deserializer=<sagemaker.base_deserializers.StringDeserializer object at 0x7f3b81117d50>\n",
       "output_deserializer=<sagemaker.base_deserializers.JSONDeserializer object at 0x7f3aa6066bd0>), model=None, inference_spec=<__main__.InferenceSpec object at 0x7f3b81a92210>, image_uri='763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:2.3-gpu-py311', image_config=None, vpc_config=None, model_server=<ModelServer.TORCHSERVE: 1>, model_metadata=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "19ac814b-a0cc-4a3c-b8d0-77d4e9b900a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "def create_tar_gz(directory, output_filename):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        # Walk through all files and subdirectories within 'directory' \n",
    "        for root, _, files in os.walk(directory):\n",
    "            # Iterate through each file found\n",
    "            for file in files:\n",
    "                filepath = os.path.join(root, file)\n",
    "                tarinfo = tarfile.TarInfo(name=filepath.replace(directory + '/', ''))  \n",
    "                tar.addfile(tarinfo, open(filepath, 'rb'))\n",
    "\n",
    "create_tar_gz(local_model_dir, package_dir) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8187bc48-c8b7-4c59-b8a9-2bc197e84b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3', region_name=region)  # Replace 'YOUR_REGION' with your bucket's region\n",
    "\n",
    "bucket_name = 'unsloth-finetuned'      # Name of your S3 bucket\n",
    "\n",
    "try:\n",
    "    # Upload the file\n",
    "    s3.upload_file(package_dir, bucket_name, package_dir)\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ddca7-d9fd-4cfa-82a0-edaaf5960941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
