{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c24eaf8-2f4e-4bec-89aa-ae6c02ac2430",
   "metadata": {},
   "source": [
    "# Case Study of the'TinyStories' Dataset\n",
    "The 'TinyStories' dataset is a large dataset that was generated using AI story request prompts and responses. It helps facilitate quick fine-tuning of models that were trained with real-world data - such as LLaMA. As this is a large dataset, and this data was generated with the help of AI, the data may have unintentionally been skewed or biased during generation. It is essential to examine the contents of this dataset and whether it is suitable to our applications content-moderation objectives with regard to literacy, mental-health, and creativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "062c2ff4-3e24-475b-95aa-3fbfbb3a284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "176d203f-fb16-4fd2-b16d-f8dad4516505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, countDistinct\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType\n",
    "from pyspark.accumulators import AccumulatorParam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c06983af-d363-42cb-99a0-15879a9e89f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a127284-6e5d-4520-b1fe-17fa615bc395",
   "metadata": {},
   "source": [
    "## Setting up the Distributed Processing Environment\n",
    "In this section, the spark distributed cluster is connected using AWS's elastic map reduce service via sagemaker studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bad87510-bb69-4cad-a5e7-e079a9e1fcd6",
   "metadata": {
    "kernelspec": {
     "display_name": "Python 3 (ipykernel)",
     "language": "python",
     "name": "python3"
    },
    "language_info": {
     "codemirror_mode": {
      "name": "ipython",
      "version": 3
     },
     "file_extension": ".py",
     "mimetype": "text/x-python",
     "name": "python",
     "nbconvert_exporter": "python",
     "pygments_lexer": "ipython3",
     "version": "3.11.11"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for EMR Serverless application state to become STARTED\n",
      "Waiting for EMR Serverless application state to become STARTED\n",
      "Initiating EMR Serverless connection..\n",
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>00fre1dr043tr90a</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://j-00fre1dr043tr90a.dashboard.emr-serverless.us-east-1.amazonaws.com/?authToken=eDAxjlx1SUIfct_lvwOvVBO9HbV7E9a2pSGGvLw36TSTM67jFf0IInQ_wJ0tI-xyZu1FgtT47emDAkNXTtUhelnsoQj5mgjRTcsh6X1Mv__w4JHHHTpVNouCBqF5ajywPnVar0GvFLgiIob0qNtWXxD2tO7GuLkbacavaviWULqc2hXb4YpCJrk9ZxmJZHgVqzjWkGXA_TKrzXpLq-xmH2yCvgCDLzFL8rXnOitGT4qN6prlbKjlvlM8MfajSMVRyAoINPmKcfklIQVcbTgkVS61bLBHwYsMj7r1GPLbrr6-CCqgmFUNLaAlBxrggzI47qQ-zH3E16BJMCTCsimE_N04eWy6JgG13rAteMqe9gfxcC3Mchb5tAA9PVgAfsAIFqJdsqIcq31zGXjI5f95auAkgXyzDl12CODjZ26qZ8hbYo_03rlLx8AQNzARxqiDMCQ68AVt2kPVnUZWVHxgnse39rZ94Kuq6goymLmlI_c773D11IKHXxMATOiqZUbSxBYxqp-CEoS-BudLb6saMmaFE0RaPIiG1-YD6qCQv0hwXvSDEoRgx2sUxH3h_d4CYAFSlNrNUhBnkRzIl2onLW-ohATvhLq2eEYllYmi4aHb8zskpOBQb-NASFhMB6vvwgT29tOuZQ_gPJepinYCb6r_P_FZobOTLZ86hPlFcPcj10Q58Oa6ifNNNveoTfDhi1SiiO0aD2Be0i5cEsm6UL9ZAxusDNlFUXA7wlAQvU7c_121fOoQbjZs5iXN2yqe9Pv8nUhmDu4SVZYqdU8jwmtuonaOmtsswzUJb6_5vxvvzHQgHC1LfWXnOVHcEv7tqtf7x9c4asOatJ0tgtr2Qw59OlM8j3cVI2JZmTHQODc8HiSIMzplznXOrz_3SkaYT8M4ujsk7AZSLtB2W2FjoDpVXHHkmMKAZ_P9aFtY88X6Rw.eyJraWQiOiJBUUlCQUhnTVJ1MlZyR2FFajR3bTc2dEZqMThGMUsyVFdZbkk4WXpxS0lRamg5YTZHZ0cvOWkyeGZ5dUlDWFhUVCtkdGI2VWZBQUFBb2pDQm53WUpLb1pJaHZjTkFRY0dvSUdSTUlHT0FnRUFNSUdJQmdrcWhraUc5dzBCQndFd0hnWUpZSVpJQVdVREJBRXVNQkVFREl5czJCU3AzTVJwTnJwU29BSUJFSUJidi9PVnZSNlZaUGpoY0ZEcE1jNFZBK1MwUEc2SXNUTEpIdzhDRUM3UzRzQnZSRWVGVklnNVVTNTA0clNtZGRhZDR6Nko1UjByb3hzS2tLa2tTNEtOUjZoSTZMYkRWeEMyVVFGcDJ0S2xDZVNldFBTYTM3Nk5BbUR4SlE9PSJ9\">Link</a></td><td><a target=\"_blank\" href=\"https://j-00fre1dr043tr90a.dashboard.emr-serverless.us-east-1.amazonaws.com/logs/SPARK_DRIVER/stderr.gz?authToken=eDAxWnokMcjHEo-7JtVa6H54VqoU-S206V34l0L0xaqwqGY4daJU5D5krPBu_PSP0fYdWDi0EsNI_tnSIpDfuk0OwuEJ0Xt6OppQngrT1-fdZnF5PhJXEdghGDLnr_n2muP35ddwTojfke6FgDj-QVxPRhtSckpV4EohiJRBwE8vI2Tbyl54EWzoDM8oBoW_fPUAzJihOY1wpJ_LmuuCOCO1pNiAbGZmQcP8cpvh2FWn2UAEdy68dakE4e_sxRoJnRN5eXPU03m3B3XqZOEs3iicXW946vM4zaniZonJzuoGhuxomWjnRVQwWyOyyVO-IPa94uuh38MiMkZlArej58VFBNTsrgDLnYwRIgFRtVSkVd7S5c1pmiVsJkBhWDctaFigpfGvOgLwGGJZtl1a0B6x9RaicB7W_6j7wYdLWGX5erkA6afm16rogtc2T__9OdPSRl5ObfXQ2HU2GrzGlQXRUMUZEzMu38zbIv6v4Wq1qONU0sJn06D5xeM3dRNsIk02Rm3g9TEjPyz2qBg3x2NQedppJ-Rp_HW004fhER_FL0uXZAS2kalvSCQn-2_GMoQtV4IRZFRH_ZbX2OhtGwXhC6Q_Q700_W-qoDofSM3R-Igej3cmpwl8fikJohL8KMpj1AgSVRxhLafONckv-ne3mEAG7ARQfT1y1OB1GR54TwQ_MDuA_OD1_9FsGAxopP196OJWsC-6UJ9dWqJUjkQrK2e5p6BULHip1MyPAPzxrq6vsr0IgHiXaImcmxCkREO-IglQO7A3jaNiwEZzZx6tyhcNEg-Ql1uow-bExGx6vc4XzWuIOK9JdHmJ5GYrWCdEeREew7ca4HUJ9hT0wO-VkHZp-SaTmG6xpJa8bbymKXc73xfsVzkhO_1CK8Pk6sOUCRa6KmNmIjl95FQibWLoo7F04oUmYHsn0StpyUfjVg.eyJraWQiOiJBUUlCQUhnTVJ1MlZyR2FFajR3bTc2dEZqMThGMUsyVFdZbkk4WXpxS0lRamg5YTZHZ0V0U3l3cEplRWY5OGp5Q1V1aGNQT0NBQUFBb2pDQm53WUpLb1pJaHZjTkFRY0dvSUdSTUlHT0FnRUFNSUdJQmdrcWhraUc5dzBCQndFd0hnWUpZSVpJQVdVREJBRXVNQkVFREF3cit1YkczZjQ5RWpCczBnSUJFSUJidlg0WHU3Wm1XUVVNcVFKek4zZWw1MitFdWorVUdVTUJWNURycjZLV09Da1AzYmJQRVlMYS9NL3RMdE9ORnRUR2N1YkNhbTA5VExwSzRITkNvdWlOb0xYRnZEUWkrODlJeEt4QkNlNFd3bER1LzhZdUd3NXZYMmZEalE9PSJ9\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eccf5d7cdf8849e688ae862a9607b5d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%load_ext sagemaker_studio_analytics_extension.magics\n",
    "%sm_analytics emr-serverless connect --application-id 00fra2001bfrlm09 --language python --emr-execution-role-arn arn:aws:iam::597161074694:role/service-role/AmazonEMR-ServiceRole-20250211T131858"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4d6dda3-922e-4a43-bd46-44fc1f95060a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/01 22:33:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# connecting to the spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .config(\"spark.driver.memory\", \"64g\") \\\n",
    "    .appName('spark') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a931d-75ee-47a5-b493-b699924dde3b",
   "metadata": {},
   "source": [
    "# Statistical Metrics\n",
    "In this section, the basic statistical metrics are calculated to be used for further data analysis and visualization in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76440b2b-338d-4c36-970a-a24e86d3ca5b",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7035db01-cc5e-4af1-975b-81b60cf42655",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"skeskinen/TinyStories-GPT4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9120ad9e-80ea-449b-9132-8cfc2ff045a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spark.createDataFrame(data['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3dc728-519e-4fcd-bf65-c87578ae50ce",
   "metadata": {},
   "source": [
    "## Defining the Spark Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d61094c5-1189-48aa-bc05-34a79f867ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordCountAccumulator(AccumulatorParam):\n",
    "    def zero(self, value):\n",
    "        return {}\n",
    "\n",
    "    def addInPlace(self, v1, v2):\n",
    "        # v1 is the accumulator's current state\n",
    "        # v2 is the new dictionary being added from a partition\n",
    "        for word, count in v2.items():\n",
    "            if word is not None and word.strip():\n",
    "                v1[word] = v1.get(word, 0) + count\n",
    "        return v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dedae9d-3f0d-4cad-bd6c-14015dcbe7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_count_unique(strings):\n",
    "    if strings is None:\n",
    "        return None\n",
    "    normalized_strings = [s.strip().lower() for s in strings if s is not None and s.strip()]\n",
    "    return normalized_strings\n",
    "# the sparkerized user defined function to normalize the words/features for comparison\n",
    "normalize_count_udf = udf(normalize_and_count_unique, ArrayType(StringType())) # Assuming string type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38106c55-c149-4c3b-8a3d-0ba335295174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the accumulator across partitions of the dataset based on col type\n",
    "def process_features_partition(partition_iterator, col_type):\n",
    "    # Initialize local counter for this partition\n",
    "    partition_features = {}\n",
    "    \n",
    "    # Process each row in the partition\n",
    "    for row in partition_iterator:\n",
    "        features = row[\"normalized_\" + col_type]\n",
    "        if features is not None:\n",
    "            # Handle both string and list cases\n",
    "            words = features.split() if isinstance(features, str) else features\n",
    "            \n",
    "            # Count words in this row\n",
    "            for word in words:\n",
    "                if word and word.strip():\n",
    "                    word = word.strip()\n",
    "                    partition_features[word] = partition_features.get(word, 0) + 1\n",
    "    \n",
    "    # Add the partition counts to the accumulator\n",
    "    if partition_features and col_type == 'features':\n",
    "        features_accumulator.add(partition_features)\n",
    "    elif partition_features and col_type == 'words':\n",
    "        words_accumulator.add(partition_features)\n",
    "    \n",
    "    # Return the iterator for the partition\n",
    "    return iter([1])  # Return dummy value to force evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc36795-25a2-432f-bceb-82672500d821",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8316a04-9656-49f6-a94f-c1b7342db371",
   "metadata": {},
   "source": [
    "### Adding the Normalized Columns For Words/Features\n",
    "UDF / user defined functions applied on the spark dataset should be added as columns prior to partitioning / converting the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc876450-a892-490c-b7b2-837d480c0056",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.withColumn(\"normalized_features\", normalize_count_udf(col(\"features\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06e45668-785b-4257-86a4-15ea50318eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.withColumn(\"normalized_words\", normalize_count_udf(col(\"words\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f741d88-d0c6-440b-aba9-75b5ded7fb54",
   "metadata": {},
   "source": [
    "### Partitioning the Dataset for Mapping the Accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51272978-5d80-44ad-b251-6bf18ffaf170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure proper partitioning\n",
    "num_partitions = 200  # Adjust based on your cluster size\n",
    "train = train.repartition(num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0425f99-27c4-417b-bd3f-3a524f3f337d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/01 22:38:28 WARN TaskSetManager: Stage 0 contains a task of very large size (223671 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 0:==============================================>          (13 + 3) / 16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:=====================================================>   (15 + 1) / 16]"
     ]
    }
   ],
   "source": [
    "# Verify the number of partitions\n",
    "print(f\"Number of partitions: {train.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a009cc-b815-46a6-8035-8f19df9a09a6",
   "metadata": {},
   "source": [
    "### Narrative Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ef23c57-99ec-4982-887a-43043bf26e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['features',\n",
       " 'prompt',\n",
       " 'source',\n",
       " 'story',\n",
       " 'summary',\n",
       " 'words',\n",
       " 'normalized_features',\n",
       " 'normalized_words']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fafc262-54fc-4e58-85b8-e61ed5952ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_accumulator = spark.sparkContext.accumulator(\n",
    "    {}, WordCountAccumulator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d70698a4-77e7-4fe7-9c9e-045e016ce9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Force evaluation and verify processing\n",
    "col_type = \"features\"  # or whatever column name you want to process\n",
    "total_partitions = train.rdd.mapPartitions(\n",
    "    lambda partition: process_features_partition(partition, col_type)\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cf566c7-7e6d-412c-8c13-174ac803d3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dialogue': 1470404,\n",
       " 'moralvalue': 274152,\n",
       " 'twist': 539383,\n",
       " 'foreshadowing': 250789,\n",
       " 'badending': 250300,\n",
       " 'conflict': 250696}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the final word counts\n",
    "features_accumulator.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbf14c1b-3bdf-4412-b33b-3d2124ba357f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dialogue         1470404\n",
       "moralvalue        274152\n",
       "twist             539383\n",
       "foreshadowing     250789\n",
       "badending         250300\n",
       "conflict          250696\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_occurences = pd.Series(features_accumulator.value)\n",
    "feature_occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308dc61e-afde-4215-83ed-f19f66d71218",
   "metadata": {},
   "source": [
    "### Key Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf71fa2e-0088-42cc-9f25-76a349eb8c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_accumulator = spark.sparkContext.accumulator(\n",
    "    {}, WordCountAccumulator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b578fb80-c7f7-4fba-8eae-9b928be416e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Force evaluation and verify processing\n",
    "col_type = \"words\"  # or whatever column name you want to process\n",
    "total_partitions = train.rdd.mapPartitions(\n",
    "    lambda partition: process_features_partition(partition, col_type)\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2737b45-5d55-43eb-891a-ea43257b740b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "admire      6971\n",
       "cane        2546\n",
       "bald       11245\n",
       "collect     6956\n",
       "aunt        2554\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_occurrences = pd.Series(words_accumulator.value)\n",
    "word_occurrences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83ade00e-9050-44c5-a976-0ccc465cdc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91613594-e21a-4c34-a676-84547fce40d5",
   "metadata": {},
   "source": [
    "### Saving the Metrics\n",
    "As the metrics calculated require a different computation environment for analysis, the metrics are saved to CSV for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b8140154-c187-4fae-8ea8-cc26b0870705",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_occurences.name = 'feature_occurrences'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9e310085-144c-4353-b1a5-a23ff9adf7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_occurences = feature_occurences.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3750eb43-50b8-4b6f-b710-cd50791ee7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_occurences.columns = ['feature', 'feature_occurrences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5b643aec-41c9-4699-b816-510b3eab69af",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_occurences.to_csv('feature_occurrences.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "960dd04e-a9b9-4b94-ad9a-2ebead586841",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_occurrences.name = 'word_occurrences'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ab8ad5ff-e6c0-4e39-8689-135991551864",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_occurrences = word_occurrences.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ea4a8eae-419a-4bf5-a750-a92329d5b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_occurrences.columns = ['word', 'word_occurrences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7e0e461b-4f17-43f2-aaa7-5b75c3daf8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_occurrences.to_csv('word_occurrences.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
