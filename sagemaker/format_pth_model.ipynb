{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e81b7fea-9263-4320-b855-33a45ed11058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 07:30:59.267812: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acafddc7-e998-4229-869f-95b9fc919d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d0764d-9824-4b80-a806-ece2c3d5c5ec",
   "metadata": {},
   "source": [
    "## Loading the Finetuned Model\n",
    "SAVED_MODEL is the lora huggingface repo of the model that has been fine-tuned. INFER_MODEL is the huggingface repo of the model that has the lora weights merged with the base weights for inferring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ccd85c0-1094-4fbe-b66e-dc94b47356dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_MODEL = \"Alexis-Az/Story-Generation-LlaMA-3.1-8B-10k\"\n",
    "INFER_MODEL= \"Alexis-Az/Story-Generation-Model\"\n",
    "max_seq_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edc4f67f-d59e-4802-8f21-5ed6330484d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/peft/config.py:162: UserWarning: Unexpected keyword arguments ['alpha_pattern', 'bias', 'eva_config', 'exclude_modules', 'fan_in_fan_out', 'init_lora_weights', 'layer_replication', 'layers_pattern', 'layers_to_transform', 'loftq_config', 'lora_alpha', 'lora_bias', 'lora_dropout', 'megatron_config', 'megatron_core', 'model_type', 'modules_to_save', 'r', 'rank_pattern', 'target_modules', 'use_dora', 'use_rslora'] for class PeftConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.568 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7b48afa75e469094c61927ee46bfa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   7%|6         | 388M/5.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6160522d20f449bad49fe55a5e19b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b87ef15e1064a8c9c7f96f557755c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ec377203494d1e8e104870838a03be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3bfac41f46b47729cb9e3ad59f5aed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/peft/config.py:162: UserWarning: Unexpected keyword arguments ['model_type'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a1556817d84c089c7bc593ff62468a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.12 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "adapter_model, tokenizer = unsloth.FastLanguageModel.from_pretrained(SAVED_MODEL, load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dc74521-7837-466e-9dfa-b59f315a9d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 15.61 out of 30.89 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:01<00:01, 14.00it/s]\n",
      "We will save to Disk and not RAM now.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:14<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "adapter_model.save_pretrained_merged(\"Story-Generation-LlaMA-3.1-8B-10k\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5bedc2-d549-430c-808d-19caa40d811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edae5336-6052-4fbf-a959-433cf54c0a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Finetuning Story Generation Model.ipynb'   hf_auth.ipynb\n",
      " README.md\t\t\t\t    inference_container\n",
      " Story-Generation-LlaMA-3.1-8B-10k\t    load_data.ipynb\n",
      " feature_engineering.ipynb\t\t    unsloth_compiled_cache\n",
      " format_pth_model.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93e13d2-d4e2-4ebe-8f8f-2bbc5491600f",
   "metadata": {},
   "source": [
    "## Saving the Merged Model \n",
    "The model will be pushed to the INFER_MODEL huggingface repo, and will also be saved as a model .pth file for use in the container for inferrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6831cbea-cdb5-4533-98e4-f19190c54d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75477ac5197f4cbf8e79da9045380eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pth_model = AutoModelForCausalLM.from_pretrained(\"./Story-Generation-LlaMA-3.1-8B-10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6297c232-6409-48f9-af07-52c67e783b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./Story-Generation-LlaMA-3.1-8B-10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c3be920-007f-45c3-a4cc-ca752325edd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dcc1c70246e4973b222ccc8e96d7d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Alexis-Az/Story-Generation-Model/commit/785a0842eca3e73c6ff7594dc7d1574f78353ce3', commit_message='Upload LlamaForCausalLM', commit_description='', oid='785a0842eca3e73c6ff7594dc7d1574f78353ce3', pr_url='https://huggingface.co/Alexis-Az/Story-Generation-Model/discussions/3', repo_url=RepoUrl('https://huggingface.co/Alexis-Az/Story-Generation-Model', endpoint='https://huggingface.co', repo_type='model', repo_id='Alexis-Az/Story-Generation-Model'), pr_revision='refs/pr/3', pr_num=3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pth_model.push_to_hub(\n",
    "    INFER_MODEL,\n",
    "    tokenizer=tokenizer,\n",
    "    safe_serialization=True,\n",
    "    create_pr=True,\n",
    "    max_shard_size=\"3GB\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5a05be-c7b3-4d20-8345-ae8b7a2f047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pth_model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b3f1fb-534f-49d3-8c39-afe7f878bdbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
