{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e81b7fea-9263-4320-b855-33a45ed11058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acafddc7-e998-4229-869f-95b9fc919d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d0764d-9824-4b80-a806-ece2c3d5c5ec",
   "metadata": {},
   "source": [
    "## Loading the Finetuned Model\n",
    "SAVED_MODEL is the lora huggingface repo of the model that has been fine-tuned. INFER_MODEL is the huggingface repo of the model that has the lora weights merged with the base weights for inferring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ccd85c0-1094-4fbe-b66e-dc94b47356dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_MODEL = \"Alexis-Az/Story-Generation-LlaMA-3.1-8B-10k\"\n",
    "INFER_MODEL= \"Alexis-Az/Story-Generation-Model\"\n",
    "max_seq_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc4f67f-d59e-4802-8f21-5ed6330484d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_model, tokenizer = unsloth.FastLanguageModel.from_pretrained(SAVED_MODEL, load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dc74521-7837-466e-9dfa-b59f315a9d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: You have 2 CPUs. Using `safe_serialization` is 10x slower.\n",
      "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
      "To force `safe_serialization`, set it to `None` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 7.74 out of 15.43 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:00<00:00, 34.04it/s]\n",
      "We will save to Disk and not RAM now.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:04<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving Story-Generation-LlaMA-3.1-8B-10k/pytorch_model-00001-of-00004.bin...\n",
      "Unsloth: Saving Story-Generation-LlaMA-3.1-8B-10k/pytorch_model-00002-of-00004.bin...\n",
      "Unsloth: Saving Story-Generation-LlaMA-3.1-8B-10k/pytorch_model-00003-of-00004.bin...\n",
      "Unsloth: Saving Story-Generation-LlaMA-3.1-8B-10k/pytorch_model-00004-of-00004.bin...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "adapter_model.save_pretrained_merged(\"Story-Generation-LlaMA-3.1-8B-10k\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edae5336-6052-4fbf-a959-433cf54c0a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Finetuning Story Generation Model.ipynb'   hf_auth.ipynb\n",
      " README.md\t\t\t\t    inference_container\n",
      " Story-Generation-LlaMA-3.1-8B-10k\t    load_data.ipynb\n",
      " feature_engineering.ipynb\t\t    unsloth_compiled_cache\n",
      " format_pth_model.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93e13d2-d4e2-4ebe-8f8f-2bbc5491600f",
   "metadata": {},
   "source": [
    "## Saving the Merged Model \n",
    "The model will be pushed to the INFER_MODEL huggingface repo, and will also be saved as a model .pth file for use in the container for inferrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6831cbea-cdb5-4533-98e4-f19190c54d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed85d72962b43d59de77c5d3169200f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pth_model = AutoModelForCausalLM.from_pretrained(\"./Story-Generation-LlaMA-3.1-8B-10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6297c232-6409-48f9-af07-52c67e783b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./Story-Generation-LlaMA-3.1-8B-10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c3be920-007f-45c3-a4cc-ca752325edd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1388fe00018e4874a907734f332ec9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 12 LFS files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57ce5b6d2ec43ba9648239c6b9b0f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00012.safetensors:   0%|          | 0.00/2.79G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f477b7492634467903c026c5226a483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00012.safetensors:   0%|          | 0.00/2.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1582ef13e05a48b5986ac1022dabd668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00012.safetensors:   0%|          | 0.00/2.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b4e6f405db4fa191febd755efaa3f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00012.safetensors:   0%|          | 0.00/2.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ac45fb2b2c4ea2b2d598bc686a2751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00012.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f785fb2fe14d482b82c6ef6ed8e81a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00012.safetensors:   0%|          | 0.00/2.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d26b8595adb48dabec88e1b85513b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00012.safetensors:   0%|          | 0.00/2.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad40842533a9456aae2d87632727b59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00012.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f380a8f5d0c449cb4d03945eedd7ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00012.safetensors:   0%|          | 0.00/2.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42b3aa21a1a485c901f563efccb254e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00012.safetensors:   0%|          | 0.00/2.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116fbadd60a64fd8a0744cc1eaaa57bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00012.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0237e8e2e7984843ae944522c559465e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00012.safetensors:   0%|          | 0.00/2.10G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Alexis-Az/Story-Generation-Model/commit/90ade79e95db7467f65c34e0cdab9863e95d7415', commit_message='Upload LlamaForCausalLM', commit_description='', oid='90ade79e95db7467f65c34e0cdab9863e95d7415', pr_url='https://huggingface.co/Alexis-Az/Story-Generation-Model/discussions/1', repo_url=RepoUrl('https://huggingface.co/Alexis-Az/Story-Generation-Model', endpoint='https://huggingface.co', repo_type='model', repo_id='Alexis-Az/Story-Generation-Model'), pr_revision='refs/pr/1', pr_num=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pth_model.push_to_hub(\n",
    "    INFER_MODEL,\n",
    "    tokenizer=tokenizer,\n",
    "    safe_serialization=True,\n",
    "    create_pr=True,\n",
    "    max_shard_size=\"3GB\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a338b682-a1c8-45ce-b106-bfd452a6d5c0",
   "metadata": {},
   "source": [
    "## Saving Model Artifacts Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9f9281c-59c3-4075-85ff-7c7a110271b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_path = './Story-Generation-LlaMA.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0af20f99-fba6-4f04-ae6a-c27f3b93c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_model.save_pretrained(pt_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
