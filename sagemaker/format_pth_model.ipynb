{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e81b7fea-9263-4320-b855-33a45ed11058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 04:52:48.170912: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-19 04:52:48.222262: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-19 04:52:48.238513: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-19 04:52:48.494798: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acafddc7-e998-4229-869f-95b9fc919d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ccd85c0-1094-4fbe-b66e-dc94b47356dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_MODEL = \"Alexis-Az/Story-Generation-LlaMA-3.1-8B-10k\"\n",
    "max_seq_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc4f67f-d59e-4802-8f21-5ed6330484d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_model, tokenizer = unsloth.FastLanguageModel.from_pretrained(SAVED_MODEL, load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dc74521-7837-466e-9dfa-b59f315a9d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: You have 2 CPUs. Using `safe_serialization` is 10x slower.\n",
      "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
      "To force `safe_serialization`, set it to `None` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 7.74 out of 15.43 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:00<00:00, 34.04it/s]\n",
      "We will save to Disk and not RAM now.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:04<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving Story-Generation-LlaMA-3.1-8B-10k/pytorch_model-00001-of-00004.bin...\n",
      "Unsloth: Saving Story-Generation-LlaMA-3.1-8B-10k/pytorch_model-00002-of-00004.bin...\n",
      "Unsloth: Saving Story-Generation-LlaMA-3.1-8B-10k/pytorch_model-00003-of-00004.bin...\n",
      "Unsloth: Saving Story-Generation-LlaMA-3.1-8B-10k/pytorch_model-00004-of-00004.bin...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "adapter_model.save_pretrained_merged(\"Story-Generation-LlaMA-3.1-8B-10k\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edae5336-6052-4fbf-a959-433cf54c0a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Finetuning Story Generation Model.ipynb'   hf_auth.ipynb\n",
      " README.md\t\t\t\t    inference_container\n",
      " Story-Generation-LlaMA-3.1-8B-10k\t    load_data.ipynb\n",
      " feature_engineering.ipynb\t\t    unsloth_compiled_cache\n",
      " format_pth_model.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed46a87-bfee-4a54-8394-5666a4437edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b8f8a45e7f8437b95009ed05288715c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pth_model = AutoModelForCausalLM.from_pretrained(\"./Story-Generation-LlaMA-3.1-8B-10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3be920-007f-45c3-a4cc-ca752325edd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
