{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e81b7fea-9263-4320-b855-33a45ed11058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "#from peft import LoraConfig, get_peft_model\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae60dce-3460-4f8e-bac2-0ee4d6d3a03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.11/site-packages (0.1.99)\n",
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.11/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.11/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
      "Using cached tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "368f08c9-e6ce-4c87-a931-98aa1c7494bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b545d4ff-21c4-4cd8-a226-b085e4ff24f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.48.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaab31d5-4c6f-4b1b-9c93-8da22b4daf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 00:23:20.460535: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.serve.builder.model_builder import ModelBuilder\n",
    "from sagemaker.serve.builder.schema_builder import SchemaBuilder\n",
    "from safetensors.torch import load_file\n",
    "import torch\n",
    "from sagemaker.serve.mode.function_pointers import Mode\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "from sagemaker.serve import InferenceSpec\n",
    "import boto3\n",
    "from sagemaker.serve import CustomPayloadTranslator\n",
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "from sagemaker.serve.spec.inference_spec import InferenceSpec\n",
    "from transformers import pipeline\n",
    "from sagemaker.serve import ModelServer\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "import sagemaker\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "sagemaker_session = Session()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# get execution role\n",
    "# please use execution role if you are using notebook instance or update the role arn if you are using a different role\n",
    "execution_role = get_execution_role() if get_execution_role() is not None else \"your-role-arn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba54faa9-495e-4996-af0a-5ad85307b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1ee75f0-b878-491a-a0fc-429f91be8c29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#!pip install -U accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ccd85c0-1094-4fbe-b66e-dc94b47356dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_MODEL = \"Alexis-Az/Story-Generation-LlaMA-3.1-8B-10k\"\n",
    "INFER_MODEL= \"Alexis-Az/Story-Generation-Model\"\n",
    "max_seq_length = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93e13d2-d4e2-4ebe-8f8f-2bbc5491600f",
   "metadata": {},
   "source": [
    "# The PEFT Weighted Merged Model \n",
    "In this notebook the peft/lora finetuned model that was finetuned on the tiny stories dataset was formatted successfully and unsuccessfully for deployment. The finetuned models weights were merged so that the lora weights were merged with the original weights as the first step prior to inference so that the model was making predictions based on the full knowledge. **Ultimately, the notebooks used for model deployment in Sagemaker are a work-in-progress experiment to test Sagemaker's API in python for model deployment strategy.**\n",
    "\n",
    "## Formatting Results\n",
    "\n",
    "The model was successfully formatted to both a weights dictionary file, as well as a torch.jit .pt model file. The torch.script.trace method is used to compile a model for deployment and is recommended by aws/sagemaker. The model was unsuccessfully deployed to an AWS sagemaker container using their ModelBuilder api methods however, due to multiple causes - a mismatch in model loading method using the recommended AWS image used for inference and the model class used. **Instead, to deploy the LLM, the model is deployed using HuggingFace's endpoint GUI by using the non-formatted model repo containing the merged lora weights and tokenizer.**\n",
    "\n",
    "### Note:\n",
    "It is not recommended to run this notebook in a AWS Sagemaker domain spaces environment fully. Firstly, sections of this code work given authentication so a user should be logged in to their own domain. Secondly, sections of this code contain errors due to the environment/container requested by the ModelBuilder during deployment do not align with the model loading method used within the \"Story-Generation-LlaMA-3.1-8B-10k\" model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d0764d-9824-4b80-a806-ece2c3d5c5ec",
   "metadata": {},
   "source": [
    "## Loading the Finetuned Model\n",
    "SAVED_MODEL is the lora huggingface repo of the peft model that has been fine-tuned. INFER_MODEL is the huggingface repo of the model that has the lora weights merged with the base weights for inferring. The model was pushed to the INFER_MODEL huggingface repo, and will be formatted for inference here. To have the model be able to be loaded in a HF TGI inference server, the model's flash attention should be turned off by using the cpu instead of the gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acafddc7-e998-4229-869f-95b9fc919d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc4f67f-d59e-4802-8f21-5ed6330484d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_model, tokenizer = unsloth.FastLanguageModel.from_pretrained(SAVED_MODEL, load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc74521-7837-466e-9dfa-b59f315a9d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_model.save_pretrained_merged(\"Story-Generation-LlaMA-3.1-8B-10k\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5bedc2-d549-430c-808d-19caa40d811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6831cbea-cdb5-4533-98e4-f19190c54d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_model = AutoModelForCausalLM.from_pretrained(\"Story-Generation-LlaMA-3.1-8B-10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6297c232-6409-48f9-af07-52c67e783b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Story-Generation-LlaMA-3.1-8B-10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3be920-007f-45c3-a4cc-ca752325edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_model.push_to(\n",
    "    INFER_MODEL,\n",
    "    tokenizer=tokenizer,\n",
    "    safe_serialization=True,\n",
    "    create_pr=True,\n",
    "    max_shard_size=\"3GB\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d2a7b502-f93e-40e0-9bab-a7cafe5cfef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from transformers import convert_slow_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fc7fce6b-f570-4f93-80bf-2873d28d7749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "051be26b-56b3-40b1-ac3b-8e3a94ece665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Alexis-Az/Story-Generation-Model/commit/9b912e6efbfac87878ea0dd39a26cfb3f7d876f2', commit_message='Upload tokenizer', commit_description='', oid='9b912e6efbfac87878ea0dd39a26cfb3f7d876f2', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Alexis-Az/Story-Generation-Model', endpoint='https://huggingface.co', repo_type='model', repo_id='Alexis-Az/Story-Generation-Model'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(INFER_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d686887e-932c-45a7-9f61-f342b6bd64dd",
   "metadata": {},
   "source": [
    "### Attempt #1: Scripting the Model as a .pth TorchScript file\n",
    "**Note: This section will not run unless using a container using large GPU memory and disk size to store a model in GPU memory, and to store the model file on disk.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "80cd2a9f-89bd-4ccd-9917-f09a75a1fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(INFER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baee1293-08c1-4f3f-99fc-10a8ec82634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained('./test_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "183773c1-c80d-49e5-960a-3d5b350aa232",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TritonWrapper(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name: str):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torchscript=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "            device_map='auto'\n",
    "            )\n",
    "        self.model.output_hidden_states = False\n",
    "                \n",
    "    def forward(self, input_ids):\n",
    "        self.model.eval()\n",
    "        o = self.model(input_ids, output_hidden_states=False)\n",
    "        return o[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8a10f08d-5103-4a01-b5b4-80f1bf9674b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f331d96000a845c5b29db08a2e52ed72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TritonWrapper(\n",
       "  (model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TritonWrapper(INFER_MODEL)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "37b9edd6-57ff-43a8-8745-5ff1c3cb828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMMY_TEXT = 'hi! can you tell me a story about sonic?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4a469e-eaf8-4e22-9a1e-96d0deb56f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = _tokenizer('hi! can you tell me a story about sonic?', return_tensors=\"pt\")\n",
    "output = _tokenizer.decode(model.model.generate(batch['input_ids'].cuda(), max_new_tokens=500)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8165eb3b-6745-44e5-bfaf-d4cf03baedc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>hi! can you tell me a story about sonic? sonic is a blue hedgehog who can run fast, jump high, and spin fast. he has a friend named amy, who is a pink human girl. they live in a place called green hill. sonic likes to run and play with amy, and he also likes to eat cheese. what is the bad guy\\'s name, who always tries to stop sonic and amy from having fun?\\nThe story: One sunny day in Green Hill, Sonic and Amy were running and playing together. They were laughing and having a great time. Sonic was spinning fast and Amy was jumping high. They were so happy and carefree.\\nSuddenly, they heard a loud and mean laugh. It was the bad guy, Dr. Eggman. He was a tall, green robot with a big, evil grin on his face. He was always trying to stop Sonic and Amy from having fun. He wanted to take over Green Hill and make everyone else sad and scared.\\nSonic and Amy looked at each other and knew they had to stop Dr. Eggman. They decided to team up and find a way to defeat him. They ran and jumped towards Dr. Eggman\\'s castle, where he was hiding. They were determined to save their home and their friendship.\\nAs they approached the castle, they saw Dr. Eggman waiting for them. He had a big, red egg on his head, which was his favorite thing to throw. He laughed and said, \"Ha ha ha! You can\\'t stop me, Sonic and Amy! I will show you my new plan to take over Green Hill!\"\\nSonic and Amy didn\\'t like what Dr. Eggman said. They knew they had to act fast. They charged towards him, ready to spin and jump and run. They were not going to let Dr. Eggman win. The battle between Sonic and Dr. Eggman and Amy began. Who do you think will win? \\nThe story continues... \\n(What do you think? Do you want to hear more of the story or do you want to make your own story?) \\n(Note: I can continue the story if you want, or I can stop here and let you make your own story. Just let me know what you prefer!) \\nPlease tell me what you want! \\nI\\'d love to hear more of the story! \\nI want to make my own story! \\nLet\\'s continue the story! \\nLet\\'s make a new story! \\nI don\\'t know, can you'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f9b62e2e-9732-4bf3-a171-ae05753b3f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TritonWrapper(\n",
       "  (model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eeb7e861-daea-4744-b9d3-65c79a832319",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./serve\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba22d17-2c6f-4897-bc43-149928cf5993",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb26beb8-807e-4195-9562-36926dff802b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:729: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n",
      "/opt/conda/lib/python3.11/site-packages/torch/jit/_trace.py:1303: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 1142756 / 1539072 (74.2%)\n",
      "Greatest absolute difference: 0.03125 at index (0, 11, 197) (up to 1e-05 allowed)\n",
      "Greatest relative difference: 670.2 at index (0, 11, 97898) (up to 1e-05 allowed)\n",
      "  _check_trace(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traced_model done\n"
     ]
    }
   ],
   "source": [
    "assert model.training == False\n",
    "with torch.no_grad():\n",
    "    model.model.eval()\n",
    "    traced_model = torch.jit.trace(model, batch['input_ids'])\n",
    "    print(\"traced_model done\")\n",
    "    torch.jit.save(traced_model, model_dir+f\"/{INFER_MODEL.split('/')[-1]}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8e8f3-1f15-4558-82ae-e1cd4022a7da",
   "metadata": {},
   "source": [
    "# Using Sagemaker's 'ModelBuilder' Class to Format the PyTorch Model Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dedfad-8f47-4d53-b2d0-2e3f587d9da0",
   "metadata": {},
   "source": [
    "This is the default code for the AWS SDK from HuggingFace's Deploy Action in their UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bf6cba6-08da-4679-9d85-00bd2c7c368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a30fd5d-f2f6-44f6-89f3-e17a5b15519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87d63a5-e012-4322-b7ed-166147e5a938",
   "metadata": {},
   "source": [
    "## Attempt #2: Using the Huggingface ModelBuilder object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c256ece0-b2fa-470c-b88c-0d4b900a4f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py311\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: gpu.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'Alexis-Az/Story-Generation-Model',\n",
    "\t'SM_NUM_GPUS': json.dumps(1),\n",
    "    'SAGEMAKER_TS_RESPONSE_TIMEOUT':'600', \n",
    "    'SAGEMAKER_MODEL_SERVER_TIMEOUT':'600',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\timage_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"3.0.1\"),\n",
    "\tenv=hub,\n",
    "\trole=execution_role, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5dec4fb3-76ab-4f6e-a842-ed6d56f86d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-tgi-inference-2025-02-25-01-49-24-225\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-pytorch-tgi-inference-2025-02-25-01-49-24-880\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-pytorch-tgi-inference-2025-02-25-01-49-24-880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1,\n",
    "\tinstance_type=\"ml.g5.12xlarge\",\n",
    "\tcontainer_startup_health_check_timeout=300,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1a3c860-f744-4971-b81c-808db181a554",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-tgi-inference-2025-02-25-01-49-24-880 in account 597161074694 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# send request\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m\t\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHi, what can you help me with?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/base_predictor.py:212\u001b[0m, in \u001b[0;36mPredictor.predict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id, custom_attributes, component_name)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inference_component_name:\n\u001b[1;32m    210\u001b[0m     request_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInferenceComponentName\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m inference_component_name\n\u001b[0;32m--> 212\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_runtime_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_response(response)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:569\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:1023\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-tgi-inference-2025-02-25-01-49-24-880 in account 597161074694 for more information."
     ]
    }
   ],
   "source": [
    "# send request\n",
    "predictor.predict({\n",
    "\t\"inputs\": \"Hi, what can you help me with?\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c992ae-e05e-448a-bdff-9a860776f70d",
   "metadata": {},
   "source": [
    "## Attempt #3: Using the Huggingface PiplineModelBuilder Class & Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b963bdc4-06f8-4fde-a212-b438957ee3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./serve/Story-Generation-Model.pt'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir+f\"/{INFER_MODEL.split('/')[-1]}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "277000f6-6132-4720-9ee6-f6438aadba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df6a19d2-01ef-40e2-ba30-125570b7653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceSpec(InferenceSpec):\n",
    "    def invoke(self, input_object: object, model: object):       \n",
    "        return model(input_object)\n",
    "        \n",
    "    def load(self, model_dir: str):\n",
    "        return pipeline('text-generation', \"Alexis-Az/Story-Generation-Model\", device_map='auto')\n",
    "\n",
    "inf_spec = InferenceSpec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9939bc0e-fef4-4830-b46e-5cd89ac9b8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1.post300'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ced1a32-2e8d-405b-8d1d-22f4e39f46c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "value: str = \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\"\n",
    "schema = SchemaBuilder(value,\n",
    "            {\"generated_text\": \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\\\nDaniel: Hello, Girafatron!\\\\nGirafatron: Hi, Daniel. I was just thinking about how magnificent giraffes are and how they should be worshiped by all.\\\\nDaniel: You and I think alike, Girafatron. I think all animals should be worshipped! But I guess that could be a bit impractical...\\\\nGirafatron: That\\'s true. But the giraffe is just such an amazing creature and should always be respected!\\\\nDaniel: Yes! And the way you go on about giraffes, I could tell you really love them.\\\\nGirafatron: I\\'m obsessed with them, and I\\'m glad to hear you noticed!\\\\nDaniel: I\\'\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f5a805e-15d3-4831-94be-2da7e54e6396",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_dir = str(Path(model_dir).resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b562c1de-3cc8-4458-af62-ca5d211deb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "package_dir = 'model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19fba3c5-f28a-4452-90ca-43d57ee6f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The diamondback terrapin or simply terrapin is a species of turtle native to the brackish coastal tidal marshes of the\"\n",
    "response = \"The diamondback terrapin or simply terrapin is a species of turtle native to the brackish coastal tidal marshes of the east coast.\"\n",
    "\n",
    "sample_input = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {}\n",
    "}\n",
    "\n",
    "sample_output = [\n",
    "    {\n",
    "        \"generated_text\": response\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d6c6fa8-48d3-4a3b-8e7b-72a1d99b655c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py310\n"
     ]
    }
   ],
   "source": [
    "instance_type = 'ml.c6a.12xlarge'\n",
    "image = image_uris.retrieve(region=region, framework='huggingface',base_framework_version='pytorch2.0.0', version = '4.28',image_scope='inference', instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "32f9ec3c-cc43-4730-bccb-0a05b445b097",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:2.0.0-transformers4.28.1-cpu-py310-ubuntu20.04'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a09c1938-5a2a-4aa1-b869-db8fd27cd543",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_builder = ModelBuilder(mode=Mode.SAGEMAKER_ENDPOINT,model=INFER_MODEL,schema_builder=SchemaBuilder(sample_input, sample_output), model_path='./huggingface', env_vars={\"HF_TASK\":\"text-generation\"}, dependencies={\"auto\":True}, image_uri=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f9895240-66e6-48d1-9898-87ba6a72aa39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelBuilder: INFO:     Either inference spec or model is provided. ModelBuilder is not handling MLflow model input\n",
      "ModelBuilder: WARNING:     HuggingFace JumpStart Model ID not detected. Building for HuggingFace Model ID.\n",
      "ModelBuilder: WARNING:     HuggingFace JumpStart Model ID not detected. Building for HuggingFace Model ID.\n",
      "ModelBuilder: WARNING:     HuggingFace Model ID support on model server: None is not currently supported. Defaulting to TGI\n",
      "ModelBuilder: WARNING:     CUDA is not enabled on your device. [Errno 2] No such file or directory: 'nvidia-smi'. Please run ModelBuilder on CUDA enabled hardware to deploy locally.\n",
      "ModelBuilder: WARNING:     65.40047919282824 percent of disk space used. Please consider freeing up disk space or increasing the EBS volume if you are on a SageMaker Notebook.\n",
      "ModelBuilder: WARNING:     Unable to check docker volume utilization at the expected path /var/lib/docker. [Errno 2] No such file or directory: '/var/lib/docker'\n",
      "ModelBuilder: WARNING:     CUDA is not enabled on your device. [Errno 2] No such file or directory: 'nvidia-smi'. Please run ModelBuilder on CUDA enabled hardware to deploy locally.\n"
     ]
    }
   ],
   "source": [
    "_model = model_builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac08299-c3e2-452f-9436-e7165021c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model.deploy(role=execution_role, instance_type='ml.g4dn.8xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecebf4b7-28b0-4b22-af09-4f7a6f752263",
   "metadata": {},
   "source": [
    "## Attempt 4: Using the ModelBuilder class and Compiling own Container (BYOC)\n",
    "The last attempt is the most tedious and less automated method which requires saving the model artifacts and using the sagemaker GUI to create the deployable model/the endpoint. **Note: deploy() in this section results in deployment errors potentially due to the model class used being a CAUSAL_LM model that AWS tries to load using a concrete FlashCausalLM class instead of the LLamaCausalLM class, or due to issues loading the lora weight merged model that has been finetuned using Unsloth. This section will not run unless using a container using large CPU memory.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1747df8-d6bb-41f2-95c4-b274c2ccdb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_builder = ModelBuilder(\n",
    "    mode=Mode.SAGEMAKER_ENDPOINT,\n",
    "    model_path=model_dir,\n",
    "    inference_spec=inf_spec,\n",
    "    role_arn=execution_role,\n",
    "    image_uri=image,\n",
    "    schema_builder=schema,\n",
    "    env_vars={\"HF_TASK\":\"text-generation\"},\n",
    "    model_server=ModelServer.TORCHSERVE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "725db008-f0a9-47b2-a861-b7bb3bf62b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelBuilder: INFO:     Either inference spec or model is provided. ModelBuilder is not handling MLflow model input\n",
      "ModelBuilder: WARNING:     HuggingFace JumpStart Model ID not detected. Building for HuggingFace Model ID.\n",
      "ModelBuilder: INFO:     Local instance_type ml.g5.12xlarge detected. ml.g5.12xlarge will be default when deploying to a SageMaker Endpoint. This default can be overriden in model.deploy()\n",
      "ModelBuilder: WARNING:     65.24585339053998 percent of disk space used. Please consider freeing up disk space or increasing the EBS volume if you are on a SageMaker Notebook.\n",
      "ModelBuilder: WARNING:     Unable to check docker volume utilization at the expected path /var/lib/docker. [Errno 2] No such file or directory: '/var/lib/docker'\n"
     ]
    }
   ],
   "source": [
    "_model = model_builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9833e40-0f98-4b6e-bf5b-446ec4d27a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "101497af-5734-4da4-9c35-17a00e5942e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "pigz is already the newest version (2.6-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt install pigz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d40defca-5bb8-4f2b-9000-7a0a0febcc7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./serve'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3cbb4255-58e5-4704-b4d7-514a1c3416aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56a2344a-277c-430d-941b-8daa1a55beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def create_tar_pigz(tar_filename, source_dir):\n",
    "    # = f\"tar -I pigz -cf {tar_filename} {source_dir}\"\n",
    "    #subprocess.run(command, shell=True, check=True)\n",
    "    command = f\"tar -I pigz -cf {tar_filename} {source_dir}\"\n",
    "    subprocess.run(command, shell=True, check=True) \n",
    "\n",
    "\n",
    "# packaging the model file itself as the top level 'directory'\n",
    "create_tar_pigz('../'+package_dir, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c04ed278-50b8-4627-bb5e-5682f019dfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8187bc48-c8b7-4c59-b8a9-2bc197e84b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3', region_name=region)  # Replace 'YOUR_REGION' with your bucket's region\n",
    "\n",
    "bucket_name = 'unsloth-finetuned'      # Name of your S3 bucket\n",
    "\n",
    "try:\n",
    "    # Upload the file\n",
    "    s3.upload_file(package_dir, bucket_name, package_dir)\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ddca7-d9fd-4cfa-82a0-edaaf5960941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
